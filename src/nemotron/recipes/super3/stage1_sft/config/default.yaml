run:
  data: SFTDataArtifact-sft:latest
  model: pretrain:latest
  env:
    container: gitlab-master.nvidia.com/dl/joc/nemo-ci/liding_r25.11-super-v3/train:pipe.44680568

recipe:
  _target_: megatron.bridge.recipes.nemotronh.nemotron_3_super.nemotron_3_super_finetune_config
  packed_sequence: true
  peft: null  # Disable LoRA, do full SFT

# Dataset config for packed Parquet shards from data_prep
# train.py builds FinetuningDatasetConfig directly (not HFDatasetConfig) to skip HF download
# Uses super3_packed_sft_dir for seamless config - auto-resolves to splits/train/ and splits/valid/
dataset:
  super3_packed_sft_dir: ${art:data,path}
  seq_length: ${art:data,pack_size}
  packed_sequence_specs:
    packed_sequence_size: ${art:data,pack_size}

train:
  train_iters: 1700
  global_batch_size: 4

# Model parallelism overrides only â€” architecture params come from the recipe
model:
  seq_length: ${art:data,pack_size}

scheduler:
  lr_warmup_iters: 4

logger:
  log_interval: 10
  wandb_project: ${run.wandb.project}
  wandb_entity: ${run.wandb.entity}
  wandb_exp_name: super3-sft

checkpoint:
  save: /nemo_run/sft
  save_interval: 20
  pretrained_checkpoint: ${art:model,path}
  finetune: true
