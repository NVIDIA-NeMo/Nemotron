run:
  data: SFTDataArtifact-sft:latest
  model: pretrain:latest
  env:
    container: nvcr.io/nvidian/nemo:25.11-nano-v3.rc2

recipe:
  _target_: megatron.bridge.recipes.qwen.qwen3.qwen3_8b_finetune_config
  packed_sequence: true

# Override dataset config to use packed .npy files from data_prep
# File naming: training_{pack_size}.npy, validation_{pack_size}.npy (Megatron-Bridge compatible)
dataset:
  _target_: megatron.bridge.training.config.FinetuningDatasetConfig
  dataset_root: ${art:data,path}
  seq_length: ${art:data,pack_size}
  packed_sequence_specs:
    _target_: megatron.bridge.data.datasets.packed_sequence.PackedSequenceSpecs
    packed_sequence_size: ${art:data,pack_size}
    packed_train_data_path: ${art:data,training_path}
    packed_val_data_path: ${art:data,validation_path}
    packed_metadata_path: ${art:data,metadata_path}

train:
  train_iters: 1700
  global_batch_size: 32

# model:
#   pipeline_model_parallel_size: 2

# tokenizer:
#   tokenizer_type: HuggingFaceTokenizer
#   tokenizer_model: meta-llama/Llama-3.2-1B

scheduler:
  lr_warmup_iters: 32

logger:
  log_interval: 10
  wandb_project: ${run.wandb.project}   # Set from env.toml [wandb]
  wandb_entity: ${run.wandb.entity}     # Set from env.toml [wandb]

checkpoint:
  save: /nemo_run/sft
  save_interval: 20
  pretrained_checkpoint: ${art:model,path}
