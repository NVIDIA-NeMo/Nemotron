{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph CLI Synthetic Dataset Generation\n",
        "\n",
        "This notebook demonstrates how to use **NVIDIA NeMo Data Designer** to create a synthetic dataset for training an AI agent to translate natural language queries into structured CLI tool calls.\n",
        "\n",
        "## What is NeMo Data Designer?\n",
        "\n",
        "[NeMo Data Designer](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/index.html) is a powerful synthetic data generation engine that transforms data designs into high-quality datasets. It supports:\n",
        "\n",
        "- **Sampling-based columns**: Generate values from statistical distributions (uniform, categorical, etc.)\n",
        "- **LLM-based columns**: Use language models to generate realistic text or structured outputs\n",
        "- **Expression columns**: Compute values based on other columns using Python expressions\n",
        "- **Jinja templating**: Create dynamic prompts with conditional logic\n",
        "\n",
        "## Use Case: LangGraph CLI Agent\n",
        "\n",
        "We're generating training data for an agent that can interpret natural language requests like:\n",
        "> \"Create a new project using the react-agent template\"\n",
        "\n",
        "And convert them to structured tool calls:\n",
        "```json\n",
        "{\"command\": \"new\", \"template\": \"react-agent\", \"path\": null, ...}\n",
        "```\n",
        "\n",
        "This synthetic data will enable fine-tuning an LLM to perform accurate tool-calling for the LangGraph CLI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Connect to the Data Designer Service\n",
        "\n",
        "The NeMo Microservices Python SDK provides a streamlined interface for interacting with Data Designer. The `NeMoDataDesignerClient` wrapper offers convenience methods like automatic dataset loading and `wait_until_done` functionality.\n",
        "\n",
        "We're connecting to a local Data Designer instance running via Docker Compose (see `nemo-microservices-quickstart_v25.11/` for the deployment configuration). When running locally, no API key is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nemo_microservices.data_designer.essentials import (\n",
        "    DataDesignerConfigBuilder, LLMTextColumnConfig, SamplerColumnConfig, LLMStructuredColumnConfig, SamplerType,\n",
        "    CategorySamplerParams, UniformSamplerParams,\n",
        "    ModelConfig, InferenceParameters,\n",
        "    NeMoDataDesignerClient\n",
        ")\n",
        "\n",
        "client = NeMoDataDesignerClient(base_url=\"http://localhost:8080\")  # local service (no API key needed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Design the Synthetic Data Schema\n",
        "\n",
        "This is where we define the structure of our synthetic dataset using Data Designer's [column types](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/define-your-data-columns/index.html). Our design uses three types of columns:\n",
        "\n",
        "### 2.1 Output Schema (Pydantic Model)\n",
        "\n",
        "First, we define a `CLIToolCall` Pydantic model that represents the structured output we want the LLM to generate. This enables **Structured Outputs** ‚Äî generating complex nested data objects with specific schemas rather than free-form text.\n",
        "\n",
        "### 2.2 Model Configuration\n",
        "\n",
        "We configure the LLM that will power our data generation. We're using `nvidia/nvidia-nemotron-nano-9b-v2` via NVIDIA's build.nvidia.com API. The `ModelConfig` specifies:\n",
        "- **alias**: A friendly name to reference this model in column definitions\n",
        "- **provider**: `nvidiabuild` for NVIDIA-hosted models\n",
        "- **inference_parameters**: Temperature, top_p, and max_tokens for generation control\n",
        "\n",
        "### 2.3 Sampler Columns\n",
        "\n",
        "[Sampling-based columns](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/define-your-data-columns/column-types/sampling-based-columns/index.html) generate values from statistical distributions without LLM calls:\n",
        "\n",
        "| Column | Sampler Type | Purpose |\n",
        "|--------|--------------|---------|\n",
        "| `command` | Category | Randomly select one of 5 CLI commands |\n",
        "| `template` | Category | Template names for `new` command |\n",
        "| `include_path` | Category (weighted) | Boolean with 25% chance of custom path |\n",
        "| `port` | Uniform | Random port number between 3000-9000 |\n",
        "| `no_browser` | Category (weighted) | Boolean with 20% chance of true |\n",
        "| `watch` | Category (weighted) | Boolean with 33% chance of true |\n",
        "| `image_tag` | Category | Docker image tag options |\n",
        "| `dockerfile_path` | Category | Output path options |\n",
        "\n",
        "### 2.4 LLM-Based Columns with Jinja Templates\n",
        "\n",
        "[LLM-based columns](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/define-your-data-columns/column-types/llm-based-columns/index.html) use language models to generate content. We use [Jinja templating](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/define-your-data-columns/using-jinja-templates/index.html) to create dynamic prompts that reference sampled values:\n",
        "\n",
        "- **`input` column** (`LLMTextColumnConfig`): Generates natural language user requests based on the sampled command type and parameters\n",
        "- **`output` column** (`LLMStructuredColumnConfig`): Converts the natural language input into a structured `CLIToolCall` JSON object\n",
        "\n",
        "The Jinja conditionals (`{% if command == 'new' %}...{% endif %}`) ensure the prompt is tailored to each command type, producing realistic and contextually appropriate training examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<meta charset=\"UTF-8\">\n",
              "<style>\n",
              "pre { line-height: 125%; }\n",
              "td.linenos .normal { color: #D8DEE9; background-color: #242933; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos { color: #D8DEE9; background-color: #242933; padding-left: 5px; padding-right: 5px; }\n",
              "td.linenos .special { color: #242933; background-color: #D8DEE9; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos.special { color: #242933; background-color: #D8DEE9; padding-left: 5px; padding-right: 5px; }\n",
              ".code .hll { background-color: #3B4252 }\n",
              ".code { background: #2E3440; color: #D8DEE9 }\n",
              ".code .c { color: #616E87; font-style: italic } /* Comment */\n",
              ".code .err { color: #BF616A } /* Error */\n",
              ".code .esc { color: #D8DEE9 } /* Escape */\n",
              ".code .g { color: #D8DEE9 } /* Generic */\n",
              ".code .k { color: #81A1C1; font-weight: bold } /* Keyword */\n",
              ".code .l { color: #D8DEE9 } /* Literal */\n",
              ".code .n { color: #D8DEE9 } /* Name */\n",
              ".code .o { color: #81A1C1; font-weight: bold } /* Operator */\n",
              ".code .x { color: #D8DEE9 } /* Other */\n",
              ".code .p { color: #ECEFF4 } /* Punctuation */\n",
              ".code .ch { color: #616E87; font-style: italic } /* Comment.Hashbang */\n",
              ".code .cm { color: #616E87; font-style: italic } /* Comment.Multiline */\n",
              ".code .cp { color: #5E81AC; font-style: italic } /* Comment.Preproc */\n",
              ".code .cpf { color: #616E87; font-style: italic } /* Comment.PreprocFile */\n",
              ".code .c1 { color: #616E87; font-style: italic } /* Comment.Single */\n",
              ".code .cs { color: #616E87; font-style: italic } /* Comment.Special */\n",
              ".code .gd { color: #BF616A } /* Generic.Deleted */\n",
              ".code .ge { color: #D8DEE9; font-style: italic } /* Generic.Emph */\n",
              ".code .ges { color: #D8DEE9; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
              ".code .gr { color: #BF616A } /* Generic.Error */\n",
              ".code .gh { color: #88C0D0; font-weight: bold } /* Generic.Heading */\n",
              ".code .gi { color: #A3BE8C } /* Generic.Inserted */\n",
              ".code .go { color: #D8DEE9 } /* Generic.Output */\n",
              ".code .gp { color: #616E88; font-weight: bold } /* Generic.Prompt */\n",
              ".code .gs { color: #D8DEE9; font-weight: bold } /* Generic.Strong */\n",
              ".code .gu { color: #88C0D0; font-weight: bold } /* Generic.Subheading */\n",
              ".code .gt { color: #BF616A } /* Generic.Traceback */\n",
              ".code .kc { color: #81A1C1; font-weight: bold } /* Keyword.Constant */\n",
              ".code .kd { color: #81A1C1; font-weight: bold } /* Keyword.Declaration */\n",
              ".code .kn { color: #81A1C1; font-weight: bold } /* Keyword.Namespace */\n",
              ".code .kp { color: #81A1C1 } /* Keyword.Pseudo */\n",
              ".code .kr { color: #81A1C1; font-weight: bold } /* Keyword.Reserved */\n",
              ".code .kt { color: #81A1C1 } /* Keyword.Type */\n",
              ".code .ld { color: #D8DEE9 } /* Literal.Date */\n",
              ".code .m { color: #B48EAD } /* Literal.Number */\n",
              ".code .s { color: #A3BE8C } /* Literal.String */\n",
              ".code .na { color: #8FBCBB } /* Name.Attribute */\n",
              ".code .nb { color: #81A1C1 } /* Name.Builtin */\n",
              ".code .nc { color: #8FBCBB } /* Name.Class */\n",
              ".code .no { color: #8FBCBB } /* Name.Constant */\n",
              ".code .nd { color: #D08770 } /* Name.Decorator */\n",
              ".code .ni { color: #D08770 } /* Name.Entity */\n",
              ".code .ne { color: #BF616A } /* Name.Exception */\n",
              ".code .nf { color: #88C0D0 } /* Name.Function */\n",
              ".code .nl { color: #D8DEE9 } /* Name.Label */\n",
              ".code .nn { color: #8FBCBB } /* Name.Namespace */\n",
              ".code .nx { color: #D8DEE9 } /* Name.Other */\n",
              ".code .py { color: #D8DEE9 } /* Name.Property */\n",
              ".code .nt { color: #81A1C1 } /* Name.Tag */\n",
              ".code .nv { color: #D8DEE9 } /* Name.Variable */\n",
              ".code .ow { color: #81A1C1; font-weight: bold } /* Operator.Word */\n",
              ".code .pm { color: #ECEFF4 } /* Punctuation.Marker */\n",
              ".code .w { color: #D8DEE9 } /* Text.Whitespace */\n",
              ".code .mb { color: #B48EAD } /* Literal.Number.Bin */\n",
              ".code .mf { color: #B48EAD } /* Literal.Number.Float */\n",
              ".code .mh { color: #B48EAD } /* Literal.Number.Hex */\n",
              ".code .mi { color: #B48EAD } /* Literal.Number.Integer */\n",
              ".code .mo { color: #B48EAD } /* Literal.Number.Oct */\n",
              ".code .sa { color: #A3BE8C } /* Literal.String.Affix */\n",
              ".code .sb { color: #A3BE8C } /* Literal.String.Backtick */\n",
              ".code .sc { color: #A3BE8C } /* Literal.String.Char */\n",
              ".code .dl { color: #A3BE8C } /* Literal.String.Delimiter */\n",
              ".code .sd { color: #616E87 } /* Literal.String.Doc */\n",
              ".code .s2 { color: #A3BE8C } /* Literal.String.Double */\n",
              ".code .se { color: #EBCB8B } /* Literal.String.Escape */\n",
              ".code .sh { color: #A3BE8C } /* Literal.String.Heredoc */\n",
              ".code .si { color: #A3BE8C } /* Literal.String.Interpol */\n",
              ".code .sx { color: #A3BE8C } /* Literal.String.Other */\n",
              ".code .sr { color: #EBCB8B } /* Literal.String.Regex */\n",
              ".code .s1 { color: #A3BE8C } /* Literal.String.Single */\n",
              ".code .ss { color: #A3BE8C } /* Literal.String.Symbol */\n",
              ".code .bp { color: #81A1C1 } /* Name.Builtin.Pseudo */\n",
              ".code .fm { color: #88C0D0 } /* Name.Function.Magic */\n",
              ".code .vc { color: #D8DEE9 } /* Name.Variable.Class */\n",
              ".code .vg { color: #D8DEE9 } /* Name.Variable.Global */\n",
              ".code .vi { color: #D8DEE9 } /* Name.Variable.Instance */\n",
              ".code .vm { color: #D8DEE9 } /* Name.Variable.Magic */\n",
              ".code .il { color: #B48EAD } /* Literal.Number.Integer.Long */\n",
              "\n",
              ".code {\n",
              "  padding: 4px;\n",
              "  border: 1px solid grey;\n",
              "  border-radius: 4px;\n",
              "  max-width: 1000px;\n",
              "  width: 100%;\n",
              "  display: inline-block;\n",
              "  box-sizing: border-box;\n",
              "  text-align: left;\n",
              "  vertical-align: top;\n",
              "  line-height: normal;\n",
              "  overflow-x: auto;\n",
              "}\n",
              "\n",
              ".code pre {\n",
              "  white-space: pre-wrap;       /* CSS 3 */\n",
              "  white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */\n",
              "  white-space: -pre-wrap;      /* Opera 4-6 */\n",
              "  white-space: -o-pre-wrap;    /* Opera 7 */\n",
              "  word-wrap: break-word;\n",
              "  overflow-wrap: break-word;\n",
              "  margin: 0;\n",
              "}\n",
              "</style>\n",
              "<div class=\"code\"><pre><span></span><span class=\"n\">DataDesignerConfigBuilder</span><span class=\"p\">(</span>\n",
              "    <span class=\"n\">sampler_columns</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n",
              "        <span class=\"s2\">&quot;command&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;template&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;include_path&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;port&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;no_browser&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;watch&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;image_tag&quot;</span><span class=\"p\">,</span>\n",
              "        <span class=\"s2\">&quot;dockerfile_path&quot;</span>\n",
              "    <span class=\"p\">]</span>\n",
              "    <span class=\"n\">llm_text_columns</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;input&#39;</span><span class=\"p\">]</span>\n",
              "    <span class=\"n\">llm_structured_columns</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">]</span>\n",
              "<span class=\"p\">)</span>\n",
              "</pre></div>\n",
              "\n"
            ],
            "text/plain": [
              "DataDesignerConfigBuilder(\n",
              "    sampler_columns: [\n",
              "        \"command\",\n",
              "        \"template\",\n",
              "        \"include_path\",\n",
              "        \"port\",\n",
              "        \"no_browser\",\n",
              "        \"watch\",\n",
              "        \"image_tag\",\n",
              "        \"dockerfile_path\"\n",
              "    ]\n",
              "    llm_text_columns: ['input']\n",
              "    llm_structured_columns: ['output']\n",
              ")"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class CLIToolCall(BaseModel):\n",
        "    command: str = Field(..., description=\"CLI command: new, dev, up, build, or dockerfile\")\n",
        "    template: Optional[str] = Field(None, description=\"Template name for 'new' command\")\n",
        "    path: Optional[str] = Field(None, description=\"Project path for 'new' command\")\n",
        "    port: Optional[int] = Field(None, description=\"Port for 'dev' or 'up' command\")\n",
        "    no_browser: Optional[bool] = Field(None, description=\"Skip browser for 'dev' command\")\n",
        "    watch: Optional[bool] = Field(None, description=\"Watch mode for 'up' command\")\n",
        "    tag: Optional[str] = Field(None, description=\"Image tag for 'build' command\")\n",
        "    output_path: Optional[str] = Field(None, description=\"Output path for 'dockerfile' command\")\n",
        "\n",
        "# Model config\n",
        "model_configs = [\n",
        "    ModelConfig(\n",
        "        alias=\"command-generator\",\n",
        "        provider=\"nvidiabuild\",\n",
        "        model=\"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "        inference_parameters=InferenceParameters(\n",
        "            temperature=0.5,\n",
        "            top_p=0.95,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "    )\n",
        "]\n",
        "\n",
        "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n",
        "\n",
        "# Sampler columns\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"command\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[\"new\", \"dev\", \"up\", \"build\", \"dockerfile\"])\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"template\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[\"basic\", \"react-agent\", \"memory-agent\", \"retrieval-agent\", \"data-enrichment\"])\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"include_path\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[True, False], weights=[1, 3])\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"port\",\n",
        "        sampler_type=SamplerType.UNIFORM,\n",
        "        params=UniformSamplerParams(low=3000, high=9000),\n",
        "        convert_to=\"int\"\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"no_browser\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[True, False], weights=[1, 4])\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"watch\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[True, False], weights=[1, 2])\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"image_tag\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[\"myapp:latest\", \"latest\", \"langgraph-app:v1\"])\n",
        "    )\n",
        ")\n",
        "\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"dockerfile_path\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(values=[\"Dockerfile\", \"Dockerfile.custom\", \"docker/Dockerfile\"])\n",
        "    )\n",
        ")\n",
        "\n",
        "# Input column with Jinja conditionals\n",
        "config_builder.add_column(\n",
        "    LLMTextColumnConfig(\n",
        "        name=\"input\",\n",
        "        model_alias=\"command-generator\",\n",
        "        prompt=(\n",
        "            \"Generate a natural user request for the LangGraph CLI.\\n\\n\"\n",
        "            \"Command: {{ command }}\\n\\n\"\n",
        "            \"{% if command == 'new' %}\"\n",
        "            \"The user wants to create a new project with the '{{ template }}' template.\"\n",
        "            \"{% if include_path %} They want it in a custom directory.{% endif %}\"\n",
        "            \"{% elif command == 'dev' %}\"\n",
        "            \"The user wants to start the dev server on port {{ port }}.\"\n",
        "            \"{% if no_browser %} They don't want to auto-open a browser.{% endif %}\"\n",
        "            \"{% elif command == 'up' %}\"\n",
        "            \"The user wants to launch the server container on port {{ port }}.\"\n",
        "            \"{% if watch %} They want to watch for code changes.{% endif %}\"\n",
        "            \"{% elif command == 'build' %}\"\n",
        "            \"The user wants to build a Docker image with tag '{{ image_tag }}'.\"\n",
        "            \"{% elif command == 'dockerfile' %}\"\n",
        "            \"The user wants to generate a Dockerfile at '{{ dockerfile_path }}'.\"\n",
        "            \"{% endif %}\\n\\n\"\n",
        "            \"Write one natural, conversational sentence.\"\n",
        "        ),\n",
        "        system_prompt=\"Output only a single sentence. No explanation.\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Output column with structured output\n",
        "config_builder.add_column(\n",
        "    LLMStructuredColumnConfig(\n",
        "        name=\"output\",\n",
        "        prompt=(\n",
        "            \"Convert this user request to a LangGraph CLI tool-call.\\n\\n\"\n",
        "            \"Command type: {{ command }}\\n\"\n",
        "            \"User request: {{ input }}\\n\\n\"\n",
        "            \"{% if command == 'new' %}\"\n",
        "            \"Set: template, and path if specified.\"\n",
        "            \"{% elif command == 'dev' %}\"\n",
        "            \"Set: port, and no_browser if specified.\"\n",
        "            \"{% elif command == 'up' %}\"\n",
        "            \"Set: port, and watch if specified.\"\n",
        "            \"{% elif command == 'build' %}\"\n",
        "            \"Set: tag.\"\n",
        "            \"{% elif command == 'dockerfile' %}\"\n",
        "            \"Set: output_path.\"\n",
        "            \"{% endif %}\\n\\n\"\n",
        "            \"Only set fields relevant to the command. Leave others as null.\"\n",
        "        ),\n",
        "        system_prompt=\"Output ONLY the user's request as a single sentence. No preamble, no quotes, no meta-commentary. /no_think\",\n",
        "        output_format=CLIToolCall,\n",
        "        model_alias=\"command-generator\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preview Data Generation\n",
        "\n",
        "Before generating a large dataset, we use the [preview](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/manage-jobs/preview-data-generation.html) feature to validate our configuration and inspect sample outputs. This follows the recommended [Data Generation Workflow](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/data-generation-workflow.html):\n",
        "\n",
        "1. **Design phase** ‚Üí Define columns and prompts\n",
        "2. **Preview** ‚Üí Generate small batches to validate quality\n",
        "3. **Iterate** ‚Üí Refine prompts and constraints\n",
        "4. **Batch generation** ‚Üí Create full dataset\n",
        "\n",
        "The preview runs the full pipeline on a small sample (5 records here), returning:\n",
        "- A Pandas DataFrame with all generated columns\n",
        "- Token usage statistics\n",
        "- Validation results\n",
        "\n",
        "This lets us verify that:\n",
        "- Natural language inputs sound realistic\n",
        "- Structured outputs conform to our Pydantic schema\n",
        "- The command-to-input-to-output pipeline produces coherent training pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[12:31:08] [INFO] ‚úÖ Validation passed\n",
            "[12:31:08] [INFO] üöÄ Starting preview generation\n",
            "[12:31:08] [INFO] ‚õìÔ∏è Sorting column configs into a Directed Acyclic Graph\n",
            "[12:31:08] [INFO] ü©∫ Running health checks for models...\n",
            "[12:31:08] [INFO]   |-- üëÄ Checking 'nvidia/nvidia-nemotron-nano-9b-v2'...\n",
            "[12:31:08] [INFO]   |-- ‚úÖ Passed!\n",
            "[12:31:08] [INFO] ‚è≥ Processing batch 1 of 1\n",
            "[12:31:08] [INFO] üé≤ Preparing samplers to generate 5 records across 8 columns\n",
            "[12:31:08] [INFO] üìù Preparing llm-text column generation\n",
            "[12:31:08] [INFO]   |-- column name: 'input'\n",
            "[12:31:08] [INFO]   |-- model config:\n",
            "{\n",
            "    \"alias\": \"command-generator\",\n",
            "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
            "    \"inference_parameters\": {\n",
            "        \"temperature\": 0.5,\n",
            "        \"top_p\": 0.95,\n",
            "        \"max_tokens\": 1000,\n",
            "        \"max_parallel_requests\": 4,\n",
            "        \"timeout\": null,\n",
            "        \"extra_body\": null\n",
            "    },\n",
            "    \"provider\": \"nvidiabuild\"\n",
            "}\n",
            "[12:31:13] [INFO] üêô Processing llm-text column 'input' with 4 concurrent workers\n",
            "[12:31:13] [INFO] üóÇÔ∏è Preparing llm-structured column generation\n",
            "[12:31:13] [INFO]   |-- column name: 'output'\n",
            "[12:31:13] [INFO]   |-- model config:\n",
            "{\n",
            "    \"alias\": \"command-generator\",\n",
            "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
            "    \"inference_parameters\": {\n",
            "        \"temperature\": 0.5,\n",
            "        \"top_p\": 0.95,\n",
            "        \"max_tokens\": 1000,\n",
            "        \"max_parallel_requests\": 4,\n",
            "        \"timeout\": null,\n",
            "        \"extra_body\": null\n",
            "    },\n",
            "    \"provider\": \"nvidiabuild\"\n",
            "}\n",
            "[12:31:13] [INFO] üêô Processing llm-structured column 'output' with 4 concurrent workers\n",
            "[12:31:14] [INFO] üìä Model usage summary:\n",
            "{\n",
            "    \"nvidia/nvidia-nemotron-nano-9b-v2\": {\n",
            "        \"token_usage\": {\n",
            "            \"prompt_tokens\": 2841,\n",
            "            \"completion_tokens\": 1094,\n",
            "            \"total_tokens\": 3935\n",
            "        },\n",
            "        \"request_usage\": {\n",
            "            \"successful_requests\": 10,\n",
            "            \"failed_requests\": 0,\n",
            "            \"total_requests\": 10\n",
            "        },\n",
            "        \"tokens_per_second\": 754,\n",
            "        \"requests_per_minute\": 114\n",
            "    }\n",
            "}\n",
            "[12:31:14] [INFO] üìê Measuring dataset column statistics:\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'command'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'template'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'include_path'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'port'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'no_browser'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'watch'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'image_tag'\n",
            "[12:31:14] [INFO]   |-- üé≤ column: 'dockerfile_path'\n",
            "[12:31:14] [INFO]   |-- üìù column: 'input'\n",
            "[12:31:14] [INFO]   |-- üóÇÔ∏è column: 'output'\n",
            "[12:31:14] [INFO] üéä Preview complete!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               input  \\\n",
            "0  Can you help me build a Docker image for my La...   \n",
            "1  Sure, you can ask, \"Can you help me generate a...   \n",
            "2  Could you help me start the server container o...   \n",
            "3  Could you generate a Dockerfile for me and sav...   \n",
            "4  Can you help me build a Docker image for my La...   \n",
            "\n",
            "                                              output  \n",
            "0  {'command': 'build', 'output_path': None, 'por...  \n",
            "1  {'command': 'dockerfile', 'output_path': 'Dock...  \n",
            "2  {'command': 'up', 'output_path': None, 'port':...  \n",
            "3  {'command': 'dockerfile', 'output_path': 'dock...  \n",
            "4  {'command': 'build', 'output_path': None, 'por...  \n"
          ]
        }
      ],
      "source": [
        "# Generate synthetic data (e.g., 50 examples) and preview\n",
        "preview = client.preview(config_builder, num_records=5)  # generate dataset\n",
        "df = preview.dataset  # Pandas DataFrame of results\n",
        "print(df[['input','output']].head(5))  # display sample pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Batch Generation Job\n",
        "\n",
        "Once we're satisfied with the preview results, we [create a batch generation job](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/manage-jobs/create-data-generation-job.html) to produce the full dataset. \n",
        "\n",
        "The `client.create()` method:\n",
        "- Validates the configuration\n",
        "- Creates an asynchronous job on the Data Designer server\n",
        "- Returns a job handle with a unique `job_id`\n",
        "\n",
        "For 1,000 records, the job processes data in batches with parallel LLM requests for efficiency. The logs show:\n",
        "- Configuration validation status\n",
        "- Job ID for tracking and retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[12:34:37] [INFO] üé® Creating Data Designer generation job\n",
            "[12:34:37] [INFO] ‚úÖ Validation passed\n",
            "[12:34:37] [INFO]   |-- job_id: job-azvfpxaxx6m6v4vkvjxawf\n"
          ]
        }
      ],
      "source": [
        "job_result = client.create(\n",
        "    config_builder,\n",
        "    num_records=1000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wait for Job Completion\n",
        "\n",
        "The `wait_until_done()` method polls the job status until generation is complete. You can also use the [job management API](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/manage-jobs/index.html) to:\n",
        "- [Get job status](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/manage-jobs/get-job-status.html)\n",
        "- [View job logs](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/manage-jobs/get-job-logs.html)\n",
        "- [Retrieve results](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/generate-data/manage-jobs/get-job-results.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[12:37:39] [INFO] ‚õìÔ∏è Sorting column configs into a Directed Acyclic Graph\n",
            "[12:37:39] [INFO] ü©∫ Running health checks for models...\n",
            "[12:37:39] [INFO]   |-- üëÄ Checking 'nvidia/nvidia-nemotron-nano-9b-v2'...\n",
            "[12:37:41] [INFO]   |-- ‚úÖ Passed!\n",
            "[12:37:41] [INFO] ‚è≥ Processing batch 1 of 2\n",
            "[12:37:41] [INFO] üé≤ Preparing samplers to generate 500 records across 8 columns\n",
            "[12:37:41] [INFO] üìù Preparing llm-text column generation\n",
            "[12:37:41] [INFO]   |-- column name: 'input'\n",
            "[12:37:41] [INFO]   |-- model config:\n",
            "{\n",
            "    \"alias\": \"command-generator\",\n",
            "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
            "    \"inference_parameters\": {\n",
            "        \"temperature\": 0.5,\n",
            "        \"top_p\": 0.95,\n",
            "        \"max_tokens\": 1000,\n",
            "        \"max_parallel_requests\": 4,\n",
            "        \"timeout\": null,\n",
            "        \"extra_body\": null\n",
            "    },\n",
            "    \"provider\": \"nvidiabuild\"\n",
            "}\n",
            "[12:37:41] [INFO] üêô Processing llm-text column 'input' with 4 concurrent workers\n"
          ]
        }
      ],
      "source": [
        "job_result.wait_until_done()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
