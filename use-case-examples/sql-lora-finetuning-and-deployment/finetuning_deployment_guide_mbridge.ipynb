{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ NVIDIA Nemotron-3-Nano LoRA Fine-Tuning Guide with Megatron Bridge\n",
        "\n",
        "This notebook walks you through **fine-tuning** the NVIDIA Nemotron-3-Nano-30B model from start to finish‚Äîusing **LoRA** (Low-Rank Adaptation) \n",
        "so you train only a small set of parameters. \n",
        "In this notebook you will train with [Megatron Bridge](https://github.com/NVIDIA-NeMo/Megatron-Bridge), part of the **NeMo** framework.\n",
        "\n",
        "[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-39PnUMhHmxbMcHKO61iQ8O5F7ZL)\n",
        "---\n",
        "\n",
        "## üìã What You're Working With\n",
        "\n",
        "| | |\n",
        "|:--:|:--|\n",
        "| ü§ñ **Model** | `NVIDIA-Nemotron-3-Nano-30B-A3B-BF16` |\n",
        "| üõ†Ô∏è **Framework** | NeMo with Megatron-Bridge |\n",
        "| üìê **Method** | LoRA (Parameter-Efficient Fine-Tuning) |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Prerequisites\n",
        "\n",
        "### üíª Hardware\n",
        "- **8√ó GPUs** ‚Äî NVIDIA H100 or A100\n",
        "- **250 GB** free storage (minimum)\n",
        "\n",
        "### üì¶ Software\n",
        "- **OS:** Ubuntu 22.04\n",
        "- **GPU driver:** 580 or newer\n",
        "- **CUDA:** 12.8 or newer\n",
        "- **NVIDIA Container Toolkit** (for Docker + GPU)\n",
        "\n",
        "---\n",
        "\n",
        "## üó∫Ô∏è Workflow at a Glance\n",
        "\n",
        "| Step | What you'll do |\n",
        "|:--:|:--|\n",
        "| **1** | üê≥ Set up the Docker environment |\n",
        "| **2** | üîÑ Convert HuggingFace model ‚Üí Megatron format |\n",
        "| **3** | üéØ Fine-tune with LoRA |\n",
        "| **4** | üîó Merge LoRA weights into the base model |\n",
        "| **5** | üì§ Export back to HuggingFace format |\n",
        "| **6** | üåê Deploy your fine-tuned model |\n",
        "\n",
        "Follow the steps below in order. Let's go! üëá\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Download the NeMo Docker Container\n",
        "\n",
        "Let's begin by obtaining the official NVIDIA NeMo container, which comes preloaded with everything needed for training.\n",
        "Before proceeding, make sure to set your NGC_API_KEY."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup NGC_API_KEY\n",
        "NGC offers a wide variety of public images, models, and datasets and you'll need to generate an API key and authenticate with NGC.\n",
        "\n",
        "To create your API key, visit: https://org.ngc.nvidia.com/setup/api-keys\n",
        "\n",
        "When generating your NGC key, make sure to enable the \"NGC Catalog\" under \"Services Included\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Put your NGC API key here\n",
        "NGC_API_KEY=\"<ENTER_YOUR_NGC_API_KEY_HERE>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Use the NGC_API_KEY set in the previous cell (run that cell first!)\n",
        "try:\n",
        "    api_key = NGC_API_KEY\n",
        "except NameError:\n",
        "    raise RuntimeError(\n",
        "        \"NGC_API_KEY is not set. Please run the cell above to set your NGC API key.\"\n",
        "    ) from None\n",
        "if not (api_key and api_key.strip()):\n",
        "    raise ValueError(\"NGC_API_KEY is empty. Please set it in the cell above.\")\n",
        "\n",
        "# Log in to NGC container registry\n",
        "result = subprocess.run(\n",
        "    [\"docker\", \"login\", \"nvcr.io\", \"-u\", \"$oauthtoken\", \"--password-stdin\"],\n",
        "    input=api_key.encode(),\n",
        "    capture_output=True,\n",
        ")\n",
        "\n",
        "print(result.stdout.decode())\n",
        "if result.returncode != 0:\n",
        "    print(\"Error:\", result.stderr.decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's download the NVIDIA Nemotron-3 Nano container from NGC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "docker pull nvcr.io/nvidia/nemo:25.11.nemotron_3_nano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Launch the Docker Container\n",
        "\n",
        "Launch the Docker container with GPU capabilities and mount your current directory as the workspace.\n",
        " \n",
        "**Key options explained:**\n",
        "- `--gpus all`: Grants the container access to all available GPUs.\n",
        "- `--ipc=host`: Shares the host‚Äôs IPC namespace for improved multi-GPU support.\n",
        "- `--network host`: Uses the host machine's network settings.\n",
        "- `-v $(pwd):/workspace`: Mounts your present directory to `/workspace` inside the container.\n",
        "- `-p 8080:8080 -p 8088:8088`: Opens essential ports for monitoring and service access.\n",
        " \n",
        "**Note:** \n",
        "Run this command in your terminal (not in Jupyter). The command will start an interactive shell session inside the container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this in your terminal:\n",
        "# docker run -it -p 8080:8080 -p 8088:8088 --rm --gpus all --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:25.11.nemotron_3_nano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "#### Note : The following steps should be executed inside the Docker container.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Set Environment Variables\n",
        "\n",
        "Set the HuggingFace model ID and specify the destination for the Megatron checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "export HF_MODEL_ID=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n",
        "export MEGATRON_MODEL_PATH=/workspace/models/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16-Mbridge\n",
        "\n",
        "echo \"HF_MODEL_ID: $HF_MODEL_ID\"\n",
        "echo \"MEGATRON_MODEL_PATH: $MEGATRON_MODEL_PATH\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Convert HuggingFace Model to Megatron Format\n",
        "\n",
        "NeMo uses Megatron-LM format for training. We need to convert the HuggingFace checkpoint to Megatron format.\n",
        "\n",
        "This step:\n",
        "- Downloads the model from HuggingFace Hub\n",
        "- Converts model weights to Megatron-compatible format\n",
        "- Saves to the specified output path\n",
        "\n",
        "**‚è±Ô∏è Note:** \n",
        "The first run downloads ~60GB+ and can take **15‚Äì60+ minutes** depending on your connection. The progress bar may stay at 0% for a while before moving‚Äîthis is normal. Do not interrupt the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /opt/Megatron-Bridge\n",
        "\n",
        "python examples/conversion/convert_checkpoints.py import \\\n",
        "  --hf-model $HF_MODEL_ID \\\n",
        "  --megatron-path $MEGATRON_MODEL_PATH \\\n",
        "  --trust-remote-code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine-tune with LoRA\n",
        "\n",
        "In this step, you will fine-tune the model using LoRA (Low-Rank Adaptation), which is an efficient technique for adapting large models with fewer trainable parameters. \n",
        "We'll use the [SQuAD dataset](https://huggingface.co/datasets/rajpurkar/squad) for this example. The SQuAD dataset is a popular benchmark for question answering, containing over 100,000 question-and-answer pairs across more than 500 diverse articles.\n",
        "\n",
        "**Key training parameters:**\n",
        "- `--peft lora`: Activates LoRA for efficient fine-tuning.\n",
        "- `train.global_batch_size=128`: Sets the total batch size combining all GPUs.\n",
        "- `train.train_iters=50`: Determines the number of training iterations.\n",
        "- `scheduler.lr_warmup_iters=10`: Number of iterations to gradually increase (warm up) the learning rate at the start of training.\n",
        "- `checkpoint.pretrained_checkpoint`: Specifies the path to the pre-converted Megatron checkpoint to start from.\n",
        "\n",
        "**Hardware note:** The sample command uses 8 GPUs (`--nproc-per-node=8`). If you have a different number of GPUs, adjust this parameter as needed.\n",
        "\n",
        "**What to expect:** \n",
        "After training, you will see output similar to:\n",
        "`validation loss at iteration 50 on validation set | lm loss value: 1.261660E-01 | lm loss PPL: 1.134470E+00 |`\n",
        "This indicates the loss and perplexity on the validation set after 50 iterations. For this setup, the training should complete in about 15 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /opt/Megatron-Bridge\n",
        "\n",
        "export ALLOW_NVLINK_FOR_NORMAL_MODE=0\n",
        "export NCCL_P2P_DISABLE=1 \n",
        "\n",
        "torchrun --nproc-per-node=8 examples/recipes/nemotron_3/finetune_nemotron_3_nano.py \\\n",
        "  --peft lora \\\n",
        "  train.global_batch_size=128 \\\n",
        "  train.train_iters=50 \\\n",
        "  scheduler.lr_warmup_iters=10 \\\n",
        "  checkpoint.pretrained_checkpoint=$MEGATRON_MODEL_PATH \\\n",
        "  model.moe_enable_deepep=False \\\n",
        "  model.moe_token_dispatcher_type=alltoall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional Step 5a ‚Äì Fine-Tune with a Custom Training Script and Dataset\n",
        "\n",
        "To use your own dataset rather than the default SQuAD dataset, you can write a custom Python training script. \\\n",
        "This approach allows you to customize the training setup to fit your specific needs. \\\n",
        "Run the steps below directly on the machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Your Dataset\n",
        "\n",
        "This step uses the **BIRD SQL** dataset (a text-to-SQL benchmark with schema, question, evidence, and SQL pairs) some helper scripts.\n",
        "These scripts apply the Nemotron chat template, filters by sequence length, and writes `training.jsonl` into a dataset directory.\n",
        "\n",
        "Run the cells below to download/prepare the dataset and save it to `dataset/training.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install into this kernel's Python (avoids user-local / system pip mismatch)\n",
        "import sys\n",
        "import subprocess\n",
        "# Ensure pip exists in the venv (some venvs are created without it)\n",
        "subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"transformers\", \"jinja2\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install datasets transformers jinja2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datasets import disable_caching\n",
        "\n",
        "disable_caching()\n",
        "\n",
        "# Ensure bird_sql is importable (repo root = /workspace in Docker, or cwd when run locally)\n",
        "workspace_root = os.environ.get(\"WORKSPACE\", os.getcwd())\n",
        "if workspace_root not in sys.path:\n",
        "    sys.path.insert(0, workspace_root)\n",
        "from bird_sql.dataset_bird import DatasetBIRD\n",
        "\n",
        "DATASET_DIR = os.environ.get(\"DATASET_DIR\", os.path.join(os.getcwd(), \"dataset\")) # automatically put in dataset in this current directory\n",
        "training_jsonl = os.path.join(DATASET_DIR, \"training.jsonl\")\n",
        "model_id = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
        "max_seq_len = 4096\n",
        "num_workers = 8\n",
        "\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Preparing BIRD training dataset...\")\n",
        "dataset = DatasetBIRD(\n",
        "    model_id_to_prep_for=model_id,\n",
        "    max_seq_len=max_seq_len,\n",
        "    num_workers=num_workers,\n",
        ").make_dataset()\n",
        "dataset = dataset.sort(\"length\")\n",
        "\n",
        "dataset.to_json(training_jsonl, orient=\"records\", lines=True, force_ascii=True)\n",
        "print(f\"Saved {len(dataset)} samples to {training_jsonl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Custom Training Script\n",
        "\n",
        "Create a Python script that configures the training with your custom dataset. \\\n",
        "This also should be run directly on the host machine, but the paths used will be the mount points in the container. \\\n",
        "No need to change paths in the script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Configuration Options\n",
        "\n",
        "You can customize training by setting environment variables:\n",
        "\n",
        "**Paths:**\n",
        "- `BASE_MODEL_PATH`: Path to converted Megatron checkpoint\n",
        "- `DATASET_DIR`: Directory containing `training.jsonl`\n",
        "- `CHECKPOINT_DIR`: Where to save training checkpoints\n",
        "\n",
        "**LoRA Parameters:**\n",
        "- `LORA_RANK`: LoRA rank (default: 16, higher = more parameters)\n",
        "- `LORA_ALPHA`: LoRA alpha scaling (default: 32)\n",
        "- `LORA_DROPOUT`: LoRA dropout rate (default: 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile custom_finetune.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Custom fine-tuning script for Nemotron-3-Nano with custom dataset.\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from megatron.bridge.recipes.nemotronh.nemotron_3_nano import (\n",
        "    nemotron_3_nano_finetune_config,\n",
        ")\n",
        "from megatron.bridge.training.config import FinetuningDatasetConfig\n",
        "from megatron.bridge.training.finetune import finetune\n",
        "from megatron.bridge.training.gpt_step import forward_step\n",
        "\n",
        "# Ensure script is launched with torchrun\n",
        "if \"LOCAL_RANK\" not in os.environ and \"RANK\" not in os.environ:\n",
        "    raise RuntimeError(\n",
        "        \"This script must be launched with torchrun. \"\n",
        "        \"Example: torchrun --nproc-per-node=8 custom_finetune.py\"\n",
        "    )\n",
        "\n",
        "# ===========================\n",
        "# CONFIGURATION PARAMETERS\n",
        "# ===========================\n",
        "\n",
        "# Paths\n",
        "BASE_MODEL_PATH = os.environ.get(\n",
        "    \"BASE_MODEL_PATH\",\n",
        "    \"/workspace/models/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16-Mbridge\"\n",
        ")\n",
        "DATASET_DIR = os.environ.get(\n",
        "    \"DATASET_DIR\",\n",
        "    os.path.join(\"/workspace/dataset\")\n",
        ")\n",
        "CHECKPOINT_DIR = os.environ.get(\n",
        "    \"CHECKPOINT_DIR\",\n",
        "    \"/opt/Megatron-Bridge/nemo_experiments/custom_run\"\n",
        ")\n",
        "\n",
        "# Training hyperparameters\n",
        "N_DEVICES = int(os.environ.get(\"N_DEVICES\", \"8\"))  # Number of GPUs\n",
        "MAX_SEQ_LEN = int(os.environ.get(\"MAX_SEQ_LEN\", \"4096\"))\n",
        "GLOBAL_BATCH_SIZE = int(os.environ.get(\"GLOBAL_BS\", \"128\"))\n",
        "PER_DEVICE_BATCH_SIZE = int(os.environ.get(\"PER_DEVICE_BS\", \"1\"))\n",
        "LEARNING_RATE = float(os.environ.get(\"LR\", \"5e-5\"))\n",
        "MIN_LR = float(os.environ.get(\"MIN_LR\", \"1e-6\"))\n",
        "WEIGHT_DECAY = float(os.environ.get(\"WEIGHT_DECAY\", \"0.001\"))\n",
        "CLIP_GRAD = float(os.environ.get(\"CLIP_GRAD\", \"1.0\"))\n",
        "WARMUP_RATIO = float(os.environ.get(\"WARMUP_RATIO\", \"0.03\"))\n",
        "EPOCHS = int(os.environ.get(\"EPOCHS\", \"3\"))\n",
        "\n",
        "# LoRA parameters\n",
        "LORA_RANK = int(os.environ.get(\"LORA_RANK\", \"16\"))\n",
        "LORA_ALPHA = int(os.environ.get(\"LORA_ALPHA\", \"32\"))\n",
        "LORA_DROPOUT = float(os.environ.get(\"LORA_DROPOUT\", \"0.05\"))\n",
        "\n",
        "\n",
        "def count_jsonl_rows(filepath: str) -> int:\n",
        "    \"\"\"Count number of lines in JSONL file.\"\"\"\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"Custom Nemotron-3-Nano Fine-tuning\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Validate paths\n",
        "    if not os.path.isdir(BASE_MODEL_PATH):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Base model path not found: {BASE_MODEL_PATH}\\n\"\n",
        "            \"Please convert the HuggingFace model to Megatron format first.\"\n",
        "        )\n",
        "    \n",
        "    training_file = os.path.join(DATASET_DIR, \"training.jsonl\")\n",
        "    if not os.path.exists(training_file):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Training data not found: {training_file}\\n\"\n",
        "            f\"Please create a training.jsonl file in {DATASET_DIR}\"\n",
        "        )\n",
        "    \n",
        "    # Calculate training steps\n",
        "    n_examples = count_jsonl_rows(training_file)\n",
        "    steps_per_epoch = math.ceil(n_examples / GLOBAL_BATCH_SIZE)\n",
        "    total_steps = EPOCHS * steps_per_epoch\n",
        "    warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "    save_interval = max(1, total_steps // 5)  # Save 5 checkpoints\n",
        "    \n",
        "    print(f\"\\nüìä Training Configuration:\")\n",
        "    print(f\"   Base model: {BASE_MODEL_PATH}\")\n",
        "    print(f\"   Dataset: {DATASET_DIR}\")\n",
        "    print(f\"   Training examples: {n_examples}\")\n",
        "    print(f\"   Epochs: {EPOCHS}\")\n",
        "    print(f\"   Total steps: {total_steps}\")\n",
        "    print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
        "    print(f\"   Global batch size: {GLOBAL_BATCH_SIZE}\")\n",
        "    print(f\"   Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
        "    print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"   LoRA rank: {LORA_RANK}\")\n",
        "    print(f\"   Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
        "    print()\n",
        "    \n",
        "    # Create base configuration\n",
        "    config = nemotron_3_nano_finetune_config(\n",
        "        seq_length=MAX_SEQ_LEN,\n",
        "        peft=\"lora\",\n",
        "        packed_sequence=False,\n",
        "        expert_model_parallelism=N_DEVICES,\n",
        "        global_batch_size=GLOBAL_BATCH_SIZE,\n",
        "        micro_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "        finetune_lr=LEARNING_RATE,\n",
        "        min_lr=MIN_LR,\n",
        "        lr_warmup_iters=warmup_steps,\n",
        "        train_iters=total_steps,\n",
        "    )\n",
        "    \n",
        "    # Configure custom dataset\n",
        "    config.dataset = FinetuningDatasetConfig(\n",
        "        dataset_root=DATASET_DIR,\n",
        "        seq_length=MAX_SEQ_LEN,\n",
        "        seed=1234,\n",
        "        num_workers=8,\n",
        "        pin_memory=True,\n",
        "        do_validation=False,  # Set to True if you have validation.jsonl\n",
        "        do_test=False,\n",
        "        dataset_kwargs={\n",
        "            \"label_key\": \"output\",\n",
        "            \"answer_only_loss\": True,\n",
        "            \"prompt_template\": \"{input} {output}\",\n",
        "            \"truncation_field\": \"input\",\n",
        "        },\n",
        "    )\n",
        "    print(f\"\\nüìã Dataset Configuration (verify these match your data):\")\n",
        "    print(f\"   label_key: 'output' (must match the target field in training.jsonl)\")\n",
        "    print(f\"   answer_only_loss: True (loss computed only on output tokens, not input)\")\n",
        "    print(f\"   prompt_template: '{{input}} {{output}}' (how fields are combined)\")\n",
        "    print(f\"   truncation_field: 'input' (input gets truncated if sequence too long)\")\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Configure model and training\n",
        "    config.model.seq_length = MAX_SEQ_LEN\n",
        "    config.model.calculate_per_token_loss = True\n",
        "    \n",
        "    # Checkpoint configuration\n",
        "    config.checkpoint.pretrained_checkpoint = BASE_MODEL_PATH\n",
        "    config.checkpoint.save_interval = save_interval\n",
        "    config.checkpoint.checkpoints_path = CHECKPOINT_DIR\n",
        "    \n",
        "    # Optimizer settings\n",
        "    config.optimizer.clip_grad = CLIP_GRAD\n",
        "    config.optimizer.weight_decay = WEIGHT_DECAY\n",
        "    \n",
        "    # LoRA configuration\n",
        "    config.peft.lora_rank = LORA_RANK\n",
        "    config.peft.lora_alpha = LORA_ALPHA\n",
        "    config.peft.lora_dropout = LORA_DROPOUT\n",
        "    \n",
        "    # Logging\n",
        "    config.logger.log_interval = 1\n",
        "    config.logger.tensorboard_dir = os.path.join(CHECKPOINT_DIR, \"tensorboard\")\n",
        "    \n",
        "    # MoE dispatcher settings (for portability)\n",
        "    config.model.moe_token_dispatcher_type = \"alltoall\"\n",
        "    config.model.moe_enable_deepep = False\n",
        "    \n",
        "    print(\"üöÄ Starting fine-tuning...\\n\")\n",
        "    \n",
        "    # Start training\n",
        "    finetune(config=config, forward_step_func=forward_step)\n",
        "    \n",
        "    print(\"\\n‚úÖ Training completed successfully!\")\n",
        "    print(f\"üìÅ Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
        "    \n",
        "    if torch.distributed.is_initialized():\n",
        "        torch.distributed.destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Launch Custom Training\n",
        "\n",
        "Now run your custom training script with torchrun\n",
        "\n",
        "**Training Parameters:**\n",
        "- `N_DEVICES`: Number of GPUs (must match `--nproc-per-node`)\n",
        "- `GLOBAL_BS`: Global batch size across all GPUs\n",
        "- `PER_DEVICE_BS`: Batch size per GPU\n",
        "- `EPOCHS`: Number of training epochs\n",
        "- `LR`: Learning rate (default: 5e-5)\n",
        "- `WARMUP_RATIO`: Fraction of steps for learning rate warmup (default: 0.03)\n",
        "\n",
        "**Run the steps below from inside the container. Your data should already be at `/workspace/dataset`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /opt/Megatron-Bridge\n",
        "\n",
        "# Set environment variables (optional - script has defaults)\n",
        "export BASE_MODEL_PATH=$MEGATRON_MODEL_PATH\n",
        "export DATASET_DIR=/workspace/dataset\n",
        "export N_DEVICES=8\n",
        "export GLOBAL_BS=128\n",
        "export EPOCHS=3\n",
        "export LR=5e-5\n",
        "export LORA_RANK=16\n",
        "\n",
        "# Launch training\n",
        "torchrun --nproc-per-node=8 /workspace/custom_finetune.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Check Training Outputs\n",
        "\n",
        "Once training is finished, check that the checkpoints have been saved successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "ls /opt/Megatron-Bridge/nemo_experiments/default/checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:**\n",
        "```\n",
        "iter_0000050\n",
        "latest_checkpointed_iteration.txt\n",
        "latest_train_state.pt\n",
        "```\n",
        "If you used custom data (5a) you will see more iterations and you can view `latest_checkpointed_iteration.txt` to view which folder you should use. \\\n",
        "Note the name of the folder of this checkpoint for the next step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Merge LoRA Weights\n",
        "\n",
        "To obtain a standalone fine-tuned model, merge the LoRA adapters into the base model.\n",
        "\n",
        "In this step:\n",
        "- The base model weights are combined with the LoRA adapter weights\n",
        "- The result is a single, merged checkpoint\n",
        "\n",
        "Again, make sure to change `--nproc_per_node=8` to your GPU count.\n",
        "\n",
        "You may need to change `--lora-checkpoint` if you used your own data \\\n",
        "to the checkpoint to your latest checkpoint you would like to use.\n",
        "\n",
        "There is no need to change the `--output` directory, but if you do make sure \\\n",
        "the path you choose is used in the remainder in the steps. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /opt/Megatron-Bridge\n",
        "\n",
        "torchrun --standalone --nproc_per_node=8 examples/peft/merge_lora.py \\\n",
        "  --hf-model-path $HF_MODEL_ID \\\n",
        "  --lora-checkpoint /opt/Megatron-Bridge/nemo_experiments/default/checkpoints/iter_0000050 \\\n",
        "  --output /workspace/models/merged_0050"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Export to HuggingFace Format\n",
        "\n",
        "Finally, convert the merged Megatron checkpoint back to HuggingFace format for easy deployment and inference.\n",
        "\n",
        "This creates a standard HuggingFace model that can be loaded with `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /opt/Megatron-Bridge\n",
        "\n",
        "python examples/conversion/convert_checkpoints.py export \\\n",
        "  --hf-model $HF_MODEL_ID \\\n",
        "  --megatron-path /workspace/models/merged_0050 \\\n",
        "  --hf-path /workspace/models/merged_0050-hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Converting to HuggingFace ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100% (6231/6231)\n",
        "\n",
        "Success: All tensors from the original checkpoint were written.\n",
        "‚úÖ Successfully exported model to: /workspace/models/merged_0050-hf\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Deploy the Fine-tuned Model with Docker Compose\n",
        "\n",
        "Now that you have your fine-tuned model, you can deploy it for inference via NVIDIA NIM (NVIDIA Inference Microservices) or vLLM. \n",
        "\n",
        "**You can exit the NeMo container that you ran commands in.**\n",
        "\n",
        "### Deployment Options\n",
        "\n",
        "The docker-compose configuration below provides two deployment options:\n",
        "\n",
        "1. **NVIDIA NIM** \n",
        "   - Uses your merged weights\n",
        "   - Supports tensor parallelism\n",
        "   - Optimized for production inference\n",
        "\n",
        "3. **vLLM - Open-source inference**\n",
        "   - Fast and memory-efficient\n",
        "   - Supports LoRA adapters\n",
        "   - PagedAttention for throughput\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before deploying, ensure:\n",
        "- Your fine-tuned model is accessible on the host machine\n",
        "- You have NGC API key (for NIM services - requires an)\n",
        "- Docker Compose is installed\n",
        "- NVIDIA Docker runtime is configured\n",
        "\n",
        "The prerequisites listed above pertain to your local environment. \n",
        "The following commands should be executed from this notebook, outside the NeMo container, directly on your host machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create docker-compose.yml\n",
        "\n",
        "Specify the HOST_MODELS_DIR where your models are stored on your host machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base directory where models are stored on your host machine. \n",
        "# merged_0050, merged_0050-hf, and NVIDIA-Nemotron-3-Nano-30B-A3B-BF16-Mbridge should be in this directory.\n",
        "# CHANGE THIS TO YOUR PATH\n",
        "HOST_MODELS_DIR = \"YOUR_MODEL_DIRECTORY_ON_YOUR_HOST_MACHINE\"\n",
        "\n",
        "# Sanity check: Your output should contain these 3 folders:\n",
        "# - merged_0050\n",
        "# - merged_0050-hf\n",
        "# - NVIDIA-Nemotron-3-Nano-30B-A3B-BF16-Mbridge\n",
        "!ls -la {HOST_MODELS_DIR}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a `docker-compose.yml` file with the following configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write docker-compose.yml with populated variables\n",
        "docker_compose_content = f\"\"\"version: '3.8'\n",
        "services:\n",
        "  # NVIDIA NIM - Deploy your fine-tuned Nemotron-3-Nano model\n",
        "  nim-nano3:\n",
        "    image: nvcr.io/nim/nvidia/nemotron-3-nano:1\n",
        "    container_name: customized-nim-nano3\n",
        "    deploy:\n",
        "      resources:\n",
        "        reservations:\n",
        "          devices:\n",
        "            - driver: nvidia\n",
        "              device_ids: ['0','1','2','3','4','5','6','7']  # Use all 8 GPUs\n",
        "              capabilities: [gpu]\n",
        "    shm_size: 128GB  # Large shared memory for multi-GPU inference\n",
        "    environment:\n",
        "      # Point to your fine-tuned model\n",
        "      - NGC_API_KEY={NGC_API_KEY}  # Required for NIM\n",
        "      - NIM_MODEL_NAME=/.cache/{HOST_MODELS_DIR}/merged_0050-hf/\n",
        "      - NIM_SERVED_MODEL_NAME=nemotron-nano-3  # Name for API requests\n",
        "      - NIM_TENSOR_PARALLEL_SIZE=8  # Split model across 2 GPUs\n",
        "      - OMPI_ALLOW_RUN_AS_ROOT=1\n",
        "      - OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1\n",
        "    volumes:\n",
        "      - \"{HOST_MODELS_DIR}:/.cache\"  # Mount models directory\n",
        "    ports:\n",
        "      - \"8007:8000\"  # API endpoint at http://localhost:8007\n",
        "    user: root\n",
        "\n",
        "  # vLLM - Open-source high-throughput inference\n",
        "  vllm:\n",
        "    image: vllm/vllm-openai:v0.13.0\n",
        "    container_name: vllm-nano8b\n",
        "    deploy:\n",
        "      resources:\n",
        "        reservations:\n",
        "          devices:\n",
        "            - driver: nvidia\n",
        "              device_ids: ['0']  # Use all GPUs\n",
        "              capabilities: [gpu]\n",
        "    shm_size: 128GB\n",
        "    environment:\n",
        "      - OMPI_ALLOW_RUN_AS_ROOT=1\n",
        "      - OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1\n",
        "    volumes:\n",
        "      - \"{HOST_MODELS_DIR}:/root/.cache\"\n",
        "    ports:\n",
        "      - \"8006:8000\"  # API endpoint at http://localhost:8006\n",
        "    user: root\n",
        "    command: [\n",
        "      \"--trust-remote-code\",\n",
        "      \"--served-model-name\", \"nemotron-nano\",\n",
        "      \"--tensor-parallel-size\", \"1\",\n",
        "      \"--model\", \"/root/.cache/merged_0050-hf\" \n",
        "    ]\n",
        "\"\"\"\n",
        "\n",
        "# Write the file\n",
        "with open('docker-compose.yml', 'w') as f:\n",
        "    f.write(docker_compose_content)\n",
        "\n",
        "print(f\"‚úÖ docker-compose.yml written successfully!\")\n",
        "print(f\"   Using HOST_MODELS_DIR: {HOST_MODELS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running this cell the docker-compose.yml file should be created in the current directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start the Inference Service\n",
        "\n",
        "Choose your deployment backend by commenting or uncommenting the relevant lines below‚Äîstart either the NIM or vLLM service as needed.\\\n",
        "Reminder: if you would like to use NVIDIA NIM a license is required. See documentation on how to obtain a free license [here](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html#nim-container-access). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Start only the NIM service with your fine-tuned model\n",
        "docker compose up -d nim-nano3\n",
        "\n",
        "# To start the vLLM service, uncomment the following line\n",
        "# docker compose up -d vllm\n",
        "\n",
        "# Check service status\n",
        "docker compose ps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment Configuration Tips\n",
        "\n",
        "**GPU Allocation:**\n",
        "- Adjust `device_ids` based on available GPUs\n",
        "- Use `nvidia-smi` to check GPU availability\n",
        "- Avoid overlapping GPU assignments between services\n",
        "\n",
        "**Tensor Parallelism:**\n",
        "- `NIM_TENSOR_PARALLEL_SIZE=2` splits model across 2 GPUs\n",
        "- Adjust based on model size and GPU memory\n",
        "- Higher TP = more GPUs, better for large models\n",
        "\n",
        "**Memory Settings:**\n",
        "- `shm_size: 128GB` provides shared memory for IPC\n",
        "- Increase if you encounter \"Bus error\" or shared memory issues\n",
        "- Must be large enough for model weights and KV cache\n",
        "\n",
        "**LoRA Configuration:**\n",
        "- `NIM_MAX_LORA_RANK=64` sets maximum adapter rank\n",
        "- `NIM_PEFT_REFRESH_INTERVAL` controls adapter reload frequency\n",
        "- Place adapters in `NIM_PEFT_SOURCE` directory\n",
        "\n",
        "**Performance Tuning:**\n",
        "- Monitor with `docker stats` or `nvidia-smi`\n",
        "- Adjust batch sizes via environment variables\n",
        "- Enable `NIM_KV_CACHE_REUSE` for better throughput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Deployed Model\n",
        "\n",
        "Once the service is running, you can test it using the OpenAI-compatible API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Send a request to **NVIDIA NIM - Nemotron-3-Nano** (Custom fine-tuned model) which is running on port 8007."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# API endpoint\n",
        "url = \"http://localhost:8007/v1/chat/completions\"\n",
        "\n",
        "# Request payload\n",
        "payload = {\n",
        "    \"model\": \"nemotron-nano-3\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Write a SQL query to list the top 5 customers by total spend. Tables: customers(id,name), orders(id,customer_id,total_amount)\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 2000\n",
        "}\n",
        "\n",
        "# Make request\n",
        "response = requests.post(url, json=payload)\n",
        "result = response.json()\n",
        "\n",
        "# Print response\n",
        "print(\"Model Response:\")\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Test with cURL\n",
        "\n",
        "You can also test using curl from the command line to the **NVIDIA NIM - Nemotron-3-Nano** customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "curl -X POST http://localhost:8007/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"model\": \"nemotron-nano-3\",\n",
        "    \"messages\": [\n",
        "      {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256\n",
        "  }'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to test the vLLM service, uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "\n",
        "# # API endpoint\n",
        "# url = \"http://localhost:8006/v1/chat/completions\"\n",
        "\n",
        "# # Request payload\n",
        "# payload = {\n",
        "#     \"model\": \"nemotron-nano\",\n",
        "#     \"messages\": [\n",
        "#         {\"role\": \"user\", \"content\": \"Hello! How can you help me today?\"}\n",
        "#     ],\n",
        "#     \"temperature\": 0.7,\n",
        "#     \"max_tokens\": 256\n",
        "# }\n",
        "\n",
        "# # Make request\n",
        "# response = requests.post(url, json=payload)\n",
        "# result = response.json()\n",
        "\n",
        "# # Print response\n",
        "# print(\"Model Response:\")\n",
        "# print(result[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor and Manage Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# View logs\n",
        "docker compose logs -f nim-nano3\n",
        "\n",
        "# Stop services\n",
        "# docker compose down\n",
        "\n",
        "# Restart a service\n",
        "# docker compose restart nim-nano3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Export Structure\n",
        "\n",
        "The exported HuggingFace model contains:\n",
        "\n",
        "```\n",
        "üìÅ /workspace/models/merged_0050-hf/\n",
        "   üìÑ config.json                         # Model configuration\n",
        "   üìÑ generation_config.json             # Generation parameters\n",
        "   üìÑ tokenizer.json                     # Tokenizer vocabulary\n",
        "   üìÑ tokenizer_config.json              # Tokenizer configuration\n",
        "   üìÑ special_tokens_map.json            # Special tokens mapping\n",
        "   üìÑ chat_template.jinja                # Chat template\n",
        "   üìÑ model.safetensors.index.json       # Model sharding index\n",
        "   üìÑ model-00001-of-00013.safetensors   # Model weights (sharded)\n",
        "   üìÑ model-00002-of-00013.safetensors\n",
        "   ... (13 shard files total)\n",
        "   üìÑ modeling_nemotron_h.py             # Custom model code\n",
        "   üìÑ configuration_nemotron_h.py        # Custom config code\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- **Model Collection:** [NVIDIA Nemotron V3 on HuggingFace](https://huggingface.co/collections/nvidia/nvidia-nemotron-v3)\n",
        "- **Base Model:** [NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16)\n",
        "- **NeMo Framework:** [NVIDIA NeMo Documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/)\n",
        "\n",
        "---\n",
        "\n",
        "## Tips and Best Practices\n",
        "\n",
        "### Training Configuration\n",
        "- **Batch Size:** Adjust `train.global_batch_size` based on GPU memory\n",
        "- **Iterations:** Increase `train.train_iters` for better convergence\n",
        "- **Learning Rate:** Tune via `optimizer.lr` and `scheduler.lr_warmup_iters`\n",
        "\n",
        "### LoRA Parameters\n",
        "- Default LoRA rank is typically 8-16 (configured in recipe)\n",
        "- Lower rank = fewer trainable parameters, faster training\n",
        "- Higher rank = more expressivity, potentially better results\n",
        "\n",
        "### GPU Requirements\n",
        "- This example uses 8 GPUs\n",
        "- For fewer GPUs: adjust `--nproc-per-node` and reduce batch size\n",
        "- Monitor GPU memory with `nvidia-smi`\n",
        "\n",
        "### Checkpoint Management\n",
        "- Checkpoints are saved to `/opt/Megatron-Bridge/nemo_experiments/`\n",
        "- Use `checkpoint.save_interval` to control checkpoint frequency\n",
        "- Keep at least 2-3 checkpoints for rollback\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Out of Memory (OOM)\n",
        "- Reduce `train.global_batch_size`\n",
        "- Enable gradient checkpointing\n",
        "- Use fewer GPUs with tensor parallelism\n",
        "\n",
        "### Slow Training\n",
        "- Check GPU utilization with `nvidia-smi`\n",
        "- Verify data loading isn't a bottleneck\n",
        "- Ensure `--ipc=host` flag is used\n",
        "\n",
        "### Conversion Errors\n",
        "- Verify HuggingFace model ID is correct\n",
        "- Check disk space for checkpoint storage\n",
        "- Ensure `--trust-remote-code` is set for custom models\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You now have a complete end-to-end workflow for fine-tuning and deploying NVIDIA Nemotron-3-Nano models!\n",
        "\n",
        "### What You've Accomplished:\n",
        "\n",
        "‚úÖ **Fine-tuning:** Trained a custom model using LoRA for efficient adaptation\n",
        "‚úÖ **Model Export:** Converted to HuggingFace format at `/workspace/models/merged_0050-hf`\n",
        "‚úÖ **Deployment:** Set up production-ready inference with NVIDIA NIM or vLLM\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "Your fine-tuned model can now be:\n",
        "\n",
        "- **Deployed:** Already configured with docker-compose for immediate use\n",
        "- **Integrated:** OpenAI-compatible API for easy integration\n",
        "- **Shared:** Upload to HuggingFace Hub for team collaboration\n",
        "- **Improved:** Further fine-tune with additional domain-specific data\n",
        "- **Scaled:** Deploy across multiple nodes for high-throughput serving\n",
        "\n",
        "### API Endpoints:\n",
        "\n",
        "Once deployed, your services are available at:\n",
        "- **NIM Nemotron-3-Nano:** `http://localhost:8007/v1/chat/completions`\n",
        "- **vLLM:** `http://localhost:8006/v1/chat/completions`\n",
        "\n",
        "Happy training and deploying! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
