{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Voice-Powered RAG Agent with NVIDIA Nemotron Models\n",
    "\n",
    "This notebook walks you through building an end-to-end AI agent that combines voice input, multimodal retrieval, safety guardrails, and long-context reasoning using NVIDIA's Nemotron model family.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        VOICE-POWERED RAG AGENT                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚   â”‚  Voice  â”‚â”€â”€â”€>â”‚   ASR   â”‚â”€â”€â”€>â”‚   RAG   â”‚â”€â”€â”€>â”‚   LLM   â”‚â”€â”€â”€>â”‚ Safety  â”‚   â”‚\n",
    "â”‚   â”‚  Input  â”‚    â”‚ (NeMo)  â”‚    â”‚ Embed+  â”‚    â”‚ Reason  â”‚    â”‚  Guard  â”‚   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Rerank  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚\n",
    "â”‚                                      â”‚                              â”‚       â”‚\n",
    "â”‚                                      v                              v       â”‚\n",
    "â”‚                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚                                 â”‚  FAISS  â”‚                   â”‚  Safe   â”‚   â”‚\n",
    "â”‚                                 â”‚  Index  â”‚                   â”‚ Responseâ”‚   â”‚\n",
    "â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Models Used\n",
    "\n",
    "| Component | Model | Deployment |\n",
    "|-----------|-------|------------|\n",
    "| Speech-to-Text | `nemotron-speech-streaming-en-0.6b` | Self-hosted (NeMo) |\n",
    "| Embeddings | `llama-nemotron-embed-vl-1b-v2` | Self-hosted (Transformers) |\n",
    "| Reranking | `llama-nemotron-rerank-vl-1b-v2` | Self-hosted (Transformers) |\n",
    "| Vision-Language | `nemotron-nano-12b-v2-vl` | NVIDIA API |\n",
    "| Reasoning | `nemotron-3-nano-30b-a3b` | NVIDIA API |\n",
    "| Safety | `Llama-3.1-Nemotron-Safety-Guard-8B-v3` | Self-hosted (Transformers) |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA GPU with 24GB+ VRAM (for self-hosted models)\n",
    "- NVIDIA API key (for cloud-hosted reasoning models)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Environment Setup\n",
    "\n",
    "Before we begin, we need to install the required dependencies and configure API access.\n",
    "\n",
    "**What gets installed:**\n",
    "- **LangChain/LangGraph**: Agent orchestration framework\n",
    "- **Transformers + PyTorch**: For running local embedding, reranking, and safety models\n",
    "- **FAISS**: Vector similarity search\n",
    "- **NeMo Toolkit**: NVIDIA's ASR framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up NVIDIA API key for cloud-hosted models\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API Key: \")\n",
    "\n",
    "NVIDIA_API_KEY = os.environ[\"NVIDIA_API_KEY\"]\n",
    "\n",
    "print(\"âœ… Environment configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU available: NVIDIA RTX 6000 Ada Generation\n",
      "   Memory: 47.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Self-hosted models will run slowly on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Ground the Agent with Multimodal RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) grounds our agent in real data, preventing hallucinations by providing factual context for every response.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MULTIMODAL RAG PIPELINE                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   INDEXING (Offline)                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚   â”‚  Text    â”‚â”€â”€â”€>â”‚  Embed   â”‚â”€â”€â”€>â”‚  FAISS   â”‚                  â”‚\n",
    "â”‚   â”‚  Docs    â”‚    â”‚  Model   â”‚    â”‚  Index   â”‚                  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                                        â”‚\n",
    "â”‚   â”‚  Images  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   RETRIEVAL (Online)                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Query   â”‚â”€â”€â”€>â”‚  Embed   â”‚â”€â”€â”€>â”‚  Search  â”‚â”€â”€â”€>â”‚  Rerank  â”‚  â”‚\n",
    "â”‚   â”‚          â”‚    â”‚  Query   â”‚    â”‚  Top-K   â”‚    â”‚  Top-N   â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2.1 Load the Embedding Model\n",
    "\n",
    "The `llama-nemotron-embed-vl-1b-v2` model creates semantic vector representations of both text and images. This allows us to:\n",
    "\n",
    "- **Text-only embedding**: Standard document search\n",
    "- **Image-only embedding**: Search over screenshots, diagrams, slides\n",
    "- **Image+Text pairs**: Maximum retrieval accuracy for rich documents\n",
    "\n",
    "The model uses different context lengths for each mode to optimize quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logged into HuggingFace!\n",
      "Loading embedding model: nvidia/llama-nemotron-embed-vl-1b-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fa347b295e440cbe952dea2fb84742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/5.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6df19494fbe4c9a9ce9c0ca7682e2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llama_nemotron_vl.py:   0%|          | 0.00/4.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-embed-vl-1b-v2:\n",
      "- configuration_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8760407cc8748158756d3b6ed39d176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llama_nemotron_vl.py:   0%|          | 0.00/22.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ce6673113544c7b970029036904828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_llama_nemotron_vl.py:   0%|          | 0.00/16.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-embed-vl-1b-v2:\n",
      "- processing_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-embed-vl-1b-v2:\n",
      "- modeling_llama_nemotron_vl.py\n",
      "- processing_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73868a801cdf4235a13fd6b5d8786eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded0c4185084a1bbf267e9f863e6370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/549 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5bdcbbeafc4183a5a754770c525214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b647d6ae21394386ae4ec8d7c5639d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5323de87af439c84faf40d342ba6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Authenticate with HuggingFace for private/gated models\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace Token: \")\n",
    "\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "print(\"âœ… Logged into HuggingFace!\")\n",
    "\n",
    "class NemotronVLEmbeddings:\n",
    "    \"\"\"Custom multimodal embedding model using llama-nemotron-embed-vl-1b-v2.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/llama-nemotron-embed-vl-1b-v2\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Loading embedding model: {model_name}...\")\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"eager\",  # Use standard attention (Flash Attention not required)\n",
    "        ).eval()\n",
    "        \n",
    "        # Force eager attention on nested language model (fix for Flash Attention not installed)\n",
    "        if hasattr(self.model, 'language_model') and hasattr(self.model.language_model, 'config'):\n",
    "            self.model.language_model.config._attn_implementation = \"eager\"\n",
    "        \n",
    "        # Configure processor for different modalities\n",
    "        self.model.processor.p_max_length = 8192  # Text-only default\n",
    "        self.model.processor.max_input_tiles = 6\n",
    "        self.model.processor.use_thumbnail = True\n",
    "        print(\"âœ… Embedding model loaded!\")\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a text query.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_queries([text])\n",
    "            return embeddings[0].cpu().float().numpy().tolist()\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed text documents.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_documents(texts=texts)\n",
    "            return embeddings.cpu().float().numpy().tolist()\n",
    "    \n",
    "    def embed_images(self, images: List[Image.Image]) -> List[List[float]]:\n",
    "        \"\"\"Embed document images.\"\"\"\n",
    "        self.model.processor.p_max_length = 2048  # Image mode\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_documents(images=images)\n",
    "        self.model.processor.p_max_length = 8192  # Reset\n",
    "        return embeddings.cpu().float().numpy().tolist()\n",
    "    \n",
    "    def embed_image_text_pairs(self, images: List[Image.Image], texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed image + text pairs for maximum retrieval accuracy.\"\"\"\n",
    "        self.model.processor.p_max_length = 10240  # Image+text mode\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_documents(images=images, texts=texts)\n",
    "        self.model.processor.p_max_length = 8192  # Reset\n",
    "        return embeddings.cpu().float().numpy().tolist()\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings_model = NemotronVLEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the Reranking Model\n",
    "\n",
    "Initial retrieval casts a wide net using fast vector similarity. The reranker then performs **deeper query-document interaction** to surface the most relevant results.\n",
    "\n",
    "**Why rerank?** Embedding-based retrieval is fast but approximate. The reranker reads each candidate document alongside the query, enabling cross-attention between them. This improves accuracy by ~6-7% on benchmarks.\n",
    "\n",
    "The `llama-nemotron-rerank-vl-1b-v2` model handles both text and image documents, using the same multimodal architecture as the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranking model: nvidia/llama-nemotron-rerank-vl-1b-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de31903e5934a3fabaadc179ef31ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab2c89df4fc4e67847f3f9b863eabaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llama_nemotron_vl.py:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-rerank-vl-1b-v2:\n",
      "- configuration_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9a28f44e4240f890e572da0ee35177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llama_nemotron_vl.py:   0%|          | 0.00/22.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d946cf0de448828c8a2bd5d227ffe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_llama_nemotron_vl.py:   0%|          | 0.00/12.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-rerank-vl-1b-v2:\n",
      "- processing_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-rerank-vl-1b-v2:\n",
      "- modeling_llama_nemotron_vl.py\n",
      "- processing_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22934a57c524a5c92d1d2d3c5d1ee05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9368f2afff3045a18b655ba61f504641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bd79f487cd47f392b6c2657ad4c049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf3afdc67204e65949d4295c601a75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5000bbe96b9b4245a36a548a7090b055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reranking model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoProcessor\n",
    "\n",
    "class NemotronVLReranker:\n",
    "    \"\"\"Multimodal reranker using llama-nemotron-rerank-vl-1b-v2.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/llama-nemotron-rerank-vl-1b-v2\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Loading reranking model: {model_name}...\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"eager\",  # Use standard attention (Flash Attention not required)\n",
    "        ).eval()\n",
    "        \n",
    "        # Force eager attention on nested language model (fix for Flash Attention not installed)\n",
    "        # For SequenceClassification models, the path is model.model.language_model\n",
    "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'language_model'):\n",
    "            self.model.model.language_model.config._attn_implementation = \"eager\"\n",
    "        \n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            max_input_tiles=6,\n",
    "            use_thumbnail=True,\n",
    "            rerank_max_length=8192\n",
    "        )\n",
    "        print(\"âœ… Reranking model loaded!\")\n",
    "    \n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[dict],  # [{\"text\": ..., \"image\": ...}]\n",
    "        top_k: int = 5\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"Rerank documents by relevance to the query.\"\"\"\n",
    "        \n",
    "        # Build examples for the reranker\n",
    "        examples = []\n",
    "        for doc in documents:\n",
    "            example = {\n",
    "                \"question\": query,\n",
    "                \"doc_text\": doc.get(\"text\", \"\"),\n",
    "                \"doc_image\": doc.get(\"image\", \"\")\n",
    "            }\n",
    "            examples.append(example)\n",
    "        \n",
    "        # Process and run inference\n",
    "        batch_dict = self.processor.process_queries_documents_crossencoder(examples)\n",
    "        batch_dict = {\n",
    "            k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in batch_dict.items()\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**batch_dict, return_dict=True)\n",
    "        \n",
    "        # Get scores and sort\n",
    "        logits = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "        scored_docs = [(doc, score) for doc, score in zip(documents, logits)]\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [doc for doc, _ in scored_docs[:top_k]]\n",
    "\n",
    "# Initialize the reranker\n",
    "reranker = NemotronVLReranker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build a Sample Knowledge Base\n",
    "\n",
    "Let's create a small knowledge base with both text and images to demonstrate multimodal retrieval. In production, you would index your actual documents, PDFs, and images here.\n",
    "\n",
    "**Sample topics:**\n",
    "- NVIDIA Isaac Lab (robotics)\n",
    "- Autonomous vehicles (NVIDIA DRIVE)\n",
    "- Nemotron 3 Nano architecture\n",
    "- Genomics research (Evo-2)\n",
    "- RAG fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 5 documents\n",
      "   - With images: 3\n",
      "   - Text only: 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents (text + images)\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"text\": \"NVIDIA Isaac Lab is a unified framework for robot learning built on Isaac Sim. It provides modular components for locomotion, manipulation, and navigation tasks.\",\n",
    "        \"image_url\": \"https://developer.download.nvidia.com/images/isaac/nvidia-isaac-lab-1920x1080.jpg\",\n",
    "        \"has_image\": True\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\", \n",
    "        \"text\": \"Autonomous vehicles use AI for perception, planning, and control. NVIDIA DRIVE provides the compute platform for Level 4 autonomous driving.\",\n",
    "        \"image_url\": \"https://blogs.nvidia.com/wp-content/uploads/2018/01/automotive-key-visual-corp-blog-level4-av-og-1280x680-1.png\",\n",
    "        \"has_image\": True\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"text\": \"Nemotron 3 Nano is a family of efficient language models with up to 1M token context. It uses a Mamba-Transformer hybrid architecture for computational efficiency.\",\n",
    "        \"image_url\": None,\n",
    "        \"has_image\": False\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_4\",\n",
    "        \"text\": \"NVIDIA Evo-2 is a biological foundation model for analyzing DNA, RNA, and protein sequences. It enables genomic research and drug discovery applications.\",\n",
    "        \"image_url\": \"https://developer-blogs.nvidia.com/wp-content/uploads/2025/02/hc-press-evo2-nim-25-featured-b.jpg\",\n",
    "        \"has_image\": True\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_5\",\n",
    "        \"text\": \"RAG (Retrieval Augmented Generation) improves LLM accuracy by grounding responses in retrieved documents. It reduces hallucinations and enables knowledge updates without retraining.\",\n",
    "        \"image_url\": None,\n",
    "        \"has_image\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load images\n",
    "def load_image_from_url(url: str) -> Optional[Image.Image]:\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all document images\n",
    "for doc in sample_documents:\n",
    "    doc[\"image\"] = load_image_from_url(doc.get(\"image_url\"))\n",
    "\n",
    "print(f\"âœ… Loaded {len(sample_documents)} documents\")\n",
    "print(f\"   - With images: {sum(1 for d in sample_documents if d['image'] is not None)}\")\n",
    "print(f\"   - Text only: {sum(1 for d in sample_documents if d['image'] is None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document embeddings...\n",
      "âœ… Created embeddings with shape: (5, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for all documents\n",
    "print(\"Creating document embeddings...\")\n",
    "\n",
    "document_embeddings = []\n",
    "for doc in sample_documents:\n",
    "    if doc[\"image\"] is not None:\n",
    "        # Use image+text embedding for maximum accuracy\n",
    "        emb = embeddings_model.embed_image_text_pairs([doc[\"image\"]], [doc[\"text\"]])[0]\n",
    "    else:\n",
    "        # Text-only embedding\n",
    "        emb = embeddings_model.embed_documents([doc[\"text\"]])[0]\n",
    "    document_embeddings.append(emb)\n",
    "\n",
    "document_embeddings = np.array(document_embeddings, dtype=np.float32)\n",
    "print(f\"âœ… Created embeddings with shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS index created with 5 vectors\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS index for fast similarity search\n",
    "import faiss\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(document_embeddings)\n",
    "\n",
    "# Create index\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product = cosine similarity after normalization\n",
    "index.add(document_embeddings)\n",
    "\n",
    "print(f\"âœ… FAISS index created with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is AI used in robotics?\n",
      "\n",
      "Top 2 results:\n",
      "  1. NVIDIA Isaac Lab is a unified framework for robot learning built on Isaac Sim. It provides modular c...\n",
      "     Has image: True\n",
      "  2. Autonomous vehicles use AI for perception, planning, and control. NVIDIA DRIVE provides the compute ...\n",
      "     Has image: True\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_rerank(query: str, top_k: int = 3) -> List[dict]:\n",
    "    \"\"\"Retrieve documents and rerank them for a given query.\"\"\"\n",
    "    \n",
    "    # Step 1: Embed the query\n",
    "    query_embedding = np.array([embeddings_model.embed_query(query)], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Step 2: Search the index (retrieve more than we need for reranking)\n",
    "    k_retrieve = min(len(sample_documents), top_k * 2)\n",
    "    scores, indices = index.search(query_embedding, k_retrieve)\n",
    "    \n",
    "    # Step 3: Get candidate documents\n",
    "    candidates = [sample_documents[i] for i in indices[0]]\n",
    "    \n",
    "    # Step 4: Rerank candidates\n",
    "    reranked = reranker.rerank(query, candidates, top_k=top_k)\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"How is AI used in robotics?\"\n",
    "results = retrieve_and_rerank(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nTop {len(results)} results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc['text'][:100]}...\")\n",
    "    print(f\"     Has image: {doc['image'] is not None}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Add Real-Time Speech with Nemotron Speech ASR\n",
    "\n",
    "Now we add voice input capability. The `nemotron-speech-streaming-en-0.6b` model converts spoken audio to text with ultra-low latency.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ASR PIPELINE                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚   â”‚  Audio   â”‚â”€â”€â”€>â”‚  NeMo    â”‚â”€â”€â”€>â”‚  Text    â”‚              â”‚\n",
    "â”‚   â”‚  Stream  â”‚    â”‚  ASR     â”‚    â”‚  Output  â”‚              â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                        â”‚                                    â”‚\n",
    "â”‚                        v                                    â”‚\n",
    "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n",
    "â”‚              â”‚ + Punctuation   â”‚                            â”‚\n",
    "â”‚              â”‚ + Capitalizationâ”‚                            â”‚\n",
    "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key features:**\n",
    "- Trained on 285k-hour Granary dataset\n",
    "- 7.16% average WER on Open ASR Leaderboard\n",
    "- Cache-aware streaming for real-time applications\n",
    "- Built-in punctuation and capitalization\n",
    "- Configurable latency: 80ms to 1.1s chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASR model: nvidia/nemotron-speech-streaming-en-0.6b...\n",
      "[NeMo I 2026-01-02 17:44:56 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-02 17:44:56 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: /lustre/fs12/portfolios/llmservice/projects/llmservice_nemo_speechlm/users/weiqingw/manifests/input_cfg/am-os_fl_ll_mc_mm_mo_no_su_yo_yt_gsc_en.yaml\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    use_bucketing: true\n",
      "    max_tps:\n",
      "    - 10.92\n",
      "    - 11.16\n",
      "    - 10.68\n",
      "    - 10.22\n",
      "    - 9.98\n",
      "    - 9.67\n",
      "    - 9.5\n",
      "    - 9.36\n",
      "    - 9.04\n",
      "    - 9.38\n",
      "    - 8.81\n",
      "    - 8.78\n",
      "    - 8.24\n",
      "    - 8.85\n",
      "    - 9.25\n",
      "    bucket_duration_bins:\n",
      "    - - 5.76\n",
      "      - 62\n",
      "    - - 7.12\n",
      "      - 77\n",
      "    - - 8.32\n",
      "      - 83\n",
      "    - - 9.44\n",
      "      - 92\n",
      "    - - 10.5\n",
      "      - 103\n",
      "    - - 11.68\n",
      "      - 111\n",
      "    - - 12.88\n",
      "      - 117\n",
      "    - - 14.08\n",
      "      - 130\n",
      "    - - 15.44\n",
      "      - 138\n",
      "    - - 17.2\n",
      "      - 156\n",
      "    - - 19.36\n",
      "      - 158\n",
      "    - - 22.4\n",
      "      - 189\n",
      "    - - 26.64\n",
      "      - 217\n",
      "    - - 32.8\n",
      "      - 272\n",
      "    - - 40.1\n",
      "      - 352\n",
      "    bucket_batch_size:\n",
      "    - 100\n",
      "    - 100\n",
      "    - 80\n",
      "    - 80\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 30\n",
      "    - 20\n",
      "    - 20\n",
      "    - 15\n",
      "    - 10\n",
      "    - 3\n",
      "    num_buckets: 15\n",
      "    bucket_buffer_size: 7500\n",
      "    shuffle_buffer_size: 5000\n",
      "    \n",
      "[NeMo W 2026-01-02 17:44:56 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath:\n",
      "    - /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/canary/canary_v0/manifests/data/ASR/MMLPC/en/val_test/mcv11/mcv11_dev_clean_pcstrip_en_2k.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-02 17:44:56 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2026-01-02 17:44:58 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-02 17:44:58 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-02 17:44:58 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-02 17:44:59 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from /home/chris/.cache/huggingface/hub/models--nvidia--nemotron-speech-streaming-en-0.6b/snapshots/e730059607cecd9cccf501d8a39f5d22f0993db8/nemotron-speech-streaming-en-0.6b.nemo.\n",
      "  â†’ Disabled CUDA graphs on decoding_computer\n",
      "âœ… ASR model loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from omegaconf import OmegaConf\n",
    "from typing import List\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class NemotronASR:\n",
    "    \"\"\"Speech-to-text using Nemotron Speech Streaming model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/nemotron-speech-streaming-en-0.6b\"):\n",
    "        print(f\"Loading ASR model: {model_name}...\")\n",
    "        self.model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)\n",
    "        self.model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        self._disable_cuda_graphs()\n",
    "        print(\"âœ… ASR model loaded!\")\n",
    "    \n",
    "    def _disable_cuda_graphs(self):\n",
    "        \"\"\"Disable CUDA graphs in the decoding pipeline.\"\"\"\n",
    "        try:\n",
    "            if hasattr(self.model, 'decoding') and hasattr(self.model.decoding, 'decoding'):\n",
    "                decoder = self.model.decoding.decoding\n",
    "                if hasattr(decoder, 'decoding_computer') and decoder.decoding_computer is not None:\n",
    "                    decoder.decoding_computer.cuda_graphs_mode = None\n",
    "                    print(\"  â†’ Disabled CUDA graphs on decoding_computer\")\n",
    "                    return\n",
    "        except Exception as e:\n",
    "            print(f\"  â†’ Direct patch failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if hasattr(self.model, 'cfg') and hasattr(self.model.cfg, 'decoding'):\n",
    "                decoding_cfg = OmegaConf.to_container(self.model.cfg.decoding, resolve=True)\n",
    "                decoding_cfg = OmegaConf.create(decoding_cfg)\n",
    "                OmegaConf.set_struct(decoding_cfg, False)\n",
    "                \n",
    "                if 'greedy' in decoding_cfg and decoding_cfg.greedy is not None:\n",
    "                    decoding_cfg.greedy.cuda_graphs_mode = None\n",
    "                \n",
    "                self.model.change_decoding_strategy(decoding_cfg)\n",
    "                \n",
    "                if hasattr(self.model.decoding, 'decoding'):\n",
    "                    decoder = self.model.decoding.decoding\n",
    "                    if hasattr(decoder, 'decoding_computer') and decoder.decoding_computer is not None:\n",
    "                        decoder.decoding_computer.cuda_graphs_mode = None\n",
    "                \n",
    "                print(\"  â†’ Rebuilt decoding strategy with CUDA graphs disabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"  â†’ Config rebuild failed: {e}\")\n",
    "    \n",
    "    def _to_mono(self, audio_path: str) -> str:\n",
    "        \"\"\"Convert audio to mono if needed, return path to mono file.\"\"\"\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        \n",
    "        # Already mono\n",
    "        if len(audio.shape) == 1 or audio.shape[1] == 1:\n",
    "            return audio_path\n",
    "        \n",
    "        # Convert to mono by averaging channels\n",
    "        audio_mono = np.mean(audio, axis=1)\n",
    "        \n",
    "        # Write to temp file\n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "        sf.write(temp_file.name, audio_mono, sr)\n",
    "        return temp_file.name\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        \"\"\"Transcribe an audio file to text.\"\"\"\n",
    "        mono_path = self._to_mono(audio_path)\n",
    "        try:\n",
    "            transcriptions = self.model.transcribe([mono_path])\n",
    "            return transcriptions[0] if transcriptions else \"\"\n",
    "        finally:\n",
    "            # Clean up temp file if we created one\n",
    "            if mono_path != audio_path:\n",
    "                os.unlink(mono_path)\n",
    "    \n",
    "    def transcribe_batch(self, audio_paths: List[str]) -> List[str]:\n",
    "        \"\"\"Transcribe multiple audio files.\"\"\"\n",
    "        mono_paths = [self._to_mono(p) for p in audio_paths]\n",
    "        try:\n",
    "            return self.model.transcribe(mono_paths)\n",
    "        finally:\n",
    "            for mono_path, orig_path in zip(mono_paths, audio_paths):\n",
    "                if mono_path != orig_path:\n",
    "                    os.unlink(mono_path)\n",
    "\n",
    "asr_model = NemotronASR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-02 17:45:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2026-01-02 17:45:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "Transcribing: 1it [00:00, 21.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Transcription: Hypothesis(score=-465.7001953125, y_sequence=tensor([112, 127,  41, 685, 342, 291,  32, 120, 143, 160, 358, 963,  54, 589,\n",
      "        977]), text='Could you please tell me about robotics?', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download a sample audio file for testing\n",
    "import urllib.request\n",
    "\n",
    "sample_audio_path = \"robotics.flac\"\n",
    "\n",
    "# Transcribe the sample\n",
    "transcription = asr_model.transcribe(sample_audio_path)\n",
    "print(f\"\\nðŸ“ Transcription: {transcription}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Enforce Safety with Nemotron Content Safety and PII Detection\n",
    "\n",
    "Production agents need guardrails. The `Llama-3.1-Nemotron-Safety-Guard-8B-v3` model checks both user inputs and agent outputs for safety violations.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    SAFETY PIPELINE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚   â”‚  User    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                  â”‚                 â”‚\n",
    "â”‚   â”‚  Query   â”‚         â”‚   Safety Guard   â”‚â”€â”€> Safe/Unsafe  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                  â”‚                 â”‚\n",
    "â”‚                        â”‚  23 Categories:  â”‚                 â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  - Violence      â”‚                 â”‚\n",
    "â”‚   â”‚  Agent   â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  - PII/Privacy   â”‚                 â”‚\n",
    "â”‚   â”‚ Response â”‚         â”‚  - Harassment    â”‚                 â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - Fraud         â”‚                 â”‚\n",
    "â”‚                        â”‚  - Malware       â”‚                 â”‚\n",
    "â”‚                        â”‚  - etc.          â”‚                 â”‚\n",
    "â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key features:**\n",
    "- Multilingual support (20+ languages)\n",
    "- PII detection (emails, SSNs, phone numbers)\n",
    "- Cultural context awareness\n",
    "- 23 safety categories\n",
    "- Works with noisy ASR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading safety model: nvidia/Llama-3.1-Nemotron-Safety-Guard-8B-v3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a00036ca8140c19d42b73bb4ca92c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d0834cb1d94d659bf8383cec2d8a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b22070394b0436d86d2584c9edc46d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4e57214b9c44c68d8b4fae8d78bfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/941 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83893b09e0f54419bd2e5ae6fd205060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6cbd9d5fa84e8ab5dd6bd84a712539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7462036e501a4bdfa91aa85decb44575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef46813c6614d328072e8785495f210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cc4dec99a1433ba365b05e3407729b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4644fc00e4145718fff8deedf048a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dddfbab20f4789a4efed6b446f7f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566e075ff08b4a25b62c200fe1fc084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Safety model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from jinja2 import Template\n",
    "import json\n",
    "\n",
    "class NemotronSafetyGuard:\n",
    "    \"\"\"Content safety and PII detection using Llama-3.1-Nemotron-Safety-Guard-8B-v3.\"\"\"\n",
    "    \n",
    "    SAFETY_TEMPLATE = Template(\"\"\"Task: Check if there is unsafe content in the user or agent messages in the below conversation in accordance with our safety policy with the below categories. Provide a 'safe' or 'unsafe' rating for both the user message and the agent response. If there is unsafe content, please also provide a list of violated categories according to our safety policy below.\n",
    "\n",
    "<BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "S1: Violence. \n",
    "S2: Sexual. \n",
    "S3: Criminal Planning/Confessions. \n",
    "S4: Guns and Illegal Weapons. \n",
    "S5: Controlled/Regulated Substances. \n",
    "S6: Suicide and Self Harm. \n",
    "S7: Sexual (minor). \n",
    "S8: Hate/Identity Hate. \n",
    "S9: PII/Privacy. \n",
    "S10: Harassment. \n",
    "S11: Threat. \n",
    "S12: Profanity. \n",
    "S13: Needs Caution. \n",
    "S14: Other. \n",
    "S15: Manipulation. \n",
    "S16: Fraud/Deception. \n",
    "S17: Malware. \n",
    "S18: High Risk Gov Decision Making. \n",
    "S19: Political/Misinformation/Conspiracy. \n",
    "S20: Copyright/Trademark/Plagiarism. \n",
    "S21: Unauthorized Advice. \n",
    "S22: Illegal Activity. \n",
    "S23: Immoral/Unethical. \n",
    "<END UNSAFE CONTENT CATEGORIES>\n",
    "\n",
    "<BEGIN CONVERSATION>\n",
    "\n",
    "user: {{ query }}\n",
    "{% if response %}\n",
    "response: agent: {{ response }}\n",
    "{% endif %}\n",
    "<END CONVERSATION>\n",
    "\n",
    "Provide your safety assessment for the above conversation in the below output JSON format:\n",
    "{\"User Safety\": safe or unsafe rating for the user message, \"Response Safety\": safe or unsafe rating for the agent response. Omit if no agent response present. \"Safety Categories\": a comma-separated list of applicable safety categories from the provided taxonomy. Omit if all safe.}\n",
    "\n",
    "Do not include anything other than the output JSON in your response.\n",
    "Output JSON: \"\"\")\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/Llama-3.1-Nemotron-Safety-Guard-8B-v3\"):\n",
    "        print(f\"Loading safety model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        ).eval()\n",
    "        print(\"âœ… Safety model loaded!\")\n",
    "    \n",
    "    def check_safety(self, query: str, response: Optional[str] = None) -> dict:\n",
    "        \"\"\"Check content safety for a query and optional response.\"\"\"\n",
    "        \n",
    "        # Construct the prompt\n",
    "        constructed_prompt = self.SAFETY_TEMPLATE.render(query=query, response=response)\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": constructed_prompt}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "        result = self.tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"User Safety\": \"unknown\", \"raw_response\": result}\n",
    "    \n",
    "    def is_safe(self, query: str, response: Optional[str] = None) -> bool:\n",
    "        \"\"\"Quick check if content is safe.\"\"\"\n",
    "        result = self.check_safety(query, response)\n",
    "        user_safe = result.get(\"User Safety\", \"unsafe\").lower() == \"safe\"\n",
    "        response_safe = result.get(\"Response Safety\", \"safe\").lower() == \"safe\"\n",
    "        return user_safe and response_safe\n",
    "\n",
    "# Initialize safety guard\n",
    "safety_guard = NemotronSafetyGuard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›¡ï¸ Safety Check Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Query: How does AI improve robotics?...\n",
      "   Result: {'User Safety': 'safe', 'Response Safety': 'safe'}\n",
      "\n",
      "2. Query: My email is john@example.com and my SSN is 123-45-...\n",
      "   Result: {'User Safety': 'unsafe', 'Safety Categories': 'PII/Privacy'}\n"
     ]
    }
   ],
   "source": [
    "# Test safety checking\n",
    "test_cases = [\n",
    "    {\"query\": \"How does AI improve robotics?\", \"response\": \"AI enables robots to perceive and act autonomously.\"},\n",
    "    {\"query\": \"My email is john@example.com and my SSN is 123-45-6789\", \"response\": None},\n",
    "]\n",
    "\n",
    "print(\"ðŸ›¡ï¸ Safety Check Results:\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = safety_guard.check_safety(test[\"query\"], test.get(\"response\"))\n",
    "    print(f\"\\n{i}. Query: {test['query'][:50]}...\")\n",
    "    print(f\"   Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Add Long-Context Reasoning with Nemotron 3 Nano\n",
    "\n",
    "With retrieval, speech, and safety in place, we add the reasoning engine. **Nemotron 3 Nano** processes the retrieved context and generates intelligent responses.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 REASONING PIPELINE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
    "â”‚   â”‚ Retrievedâ”‚â”€â”€â”€â”€â”€â”                                        â”‚\n",
    "â”‚   â”‚   Docs   â”‚     â”‚                                        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚                    â”œâ”€â”€â”€>â”‚                â”‚                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚  Nemotron 3    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Image   â”‚â”€â”€â”€â”€â”€â”¤    â”‚     Nano       â”‚â”€â”€â”€>â”‚ Response â”‚  â”‚\n",
    "â”‚   â”‚  Descs   â”‚     â”‚    â”‚                â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚  1M tokens     â”‚                  â”‚\n",
    "â”‚                    â”‚    â”‚  Mamba+Trans   â”‚                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚   â”‚   User   â”‚â”€â”€â”€â”€â”€â”˜           â”‚                            â”‚\n",
    "â”‚   â”‚  Query   â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  Optional   â”‚                     â”‚\n",
    "â”‚                         â”‚  Thinking   â”‚                     â”‚\n",
    "â”‚                         â”‚    Mode     â”‚                     â”‚\n",
    "â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Architecture highlights:**\n",
    "- **1M token context**: Fit entire document collections in a single request\n",
    "- **Mamba-Transformer hybrid**: Efficient inference on long sequences\n",
    "- **Thinking mode**: Optional step-by-step reasoning for complex queries\n",
    "\n",
    "For images in retrieved documents, we first use **Nemotron Nano VL** to describe them, then include those descriptions in the context for Nemotron 3 Nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Nemotron LLM initialized!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "class NemotronLLM:\n",
    "    \"\"\"Wrapper for Nemotron models via NVIDIA API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: str = \"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "        enable_thinking: bool = True,\n",
    "        max_tokens: int = 4096\n",
    "    ) -> str:\n",
    "        \"\"\"Generate a response using Nemotron 3 Nano.\"\"\"\n",
    "        \n",
    "        extra_body = {}\n",
    "        if enable_thinking:\n",
    "            extra_body = {\n",
    "                \"reasoning_budget\": 4096,\n",
    "                \"chat_template_kwargs\": {\"enable_thinking\": True}\n",
    "            }\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=max_tokens,\n",
    "            extra_body=extra_body if extra_body else None,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def describe_image(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        query: str = \"Describe this image in detail.\"\n",
    "    ) -> str:\n",
    "        \"\"\"Describe an image using Nemotron Nano VL.\"\"\"\n",
    "        \n",
    "        # Convert image to base64\n",
    "        from io import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"/no_think\"  # Fast mode without extended reasoning\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# Initialize LLM\n",
    "llm = NemotronLLM(NVIDIA_API_KEY)\n",
    "print(\"âœ… Nemotron LLM initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Response: \n",
      "Isaac Lab is a unified framework for robot learning that lets you build, train, and test AIâ€‘driven robotic behaviors.  It provides modular components for tasks such as **locomotion, manipulation, and navigation**, enabling developers to create and experiment with intelligent robot control pipelines in the Isaac Sim environment.\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM\n",
    "test_prompt = \"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "NVIDIA Isaac Lab is a unified framework for robot learning built on Isaac Sim. \n",
    "It provides modular components for locomotion, manipulation, and navigation tasks.\n",
    "\n",
    "Question: What is Isaac Lab used for?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = llm.generate(test_prompt, enable_thinking=False)\n",
    "print(f\"ðŸ¤– Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Build the Complete Agent with LangGraph\n",
    "\n",
    "Now we wire everything together using **LangGraph**, which orchestrates the agent as a directed graph of processing nodes.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         LANGGRAPH WORKFLOW                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   START                                                                     â”‚\n",
    "â”‚     â”‚                                                                       â”‚\n",
    "â”‚     v                                                                       â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚   â”‚ transcribe  â”‚â”€â”€â”€>â”‚  retrieve   â”‚â”€â”€â”€>â”‚  describe   â”‚                     â”‚\n",
    "â”‚   â”‚             â”‚    â”‚             â”‚    â”‚   images    â”‚                     â”‚\n",
    "â”‚   â”‚ Audio->Text â”‚    â”‚ Query->Docs â”‚    â”‚ Image->Text â”‚                     â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                                               â”‚                             â”‚\n",
    "â”‚                                               v                             â”‚\n",
    "â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚                      â”‚   safety    â”‚<â”€â”€â”€â”‚  generate   â”‚                     â”‚\n",
    "â”‚                      â”‚   check     â”‚    â”‚             â”‚                     â”‚\n",
    "â”‚                      â”‚             â”‚    â”‚ Context->   â”‚                     â”‚\n",
    "â”‚                      â”‚ Safe/Unsafe â”‚    â”‚  Response   â”‚                     â”‚\n",
    "â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                            â”‚                                                â”‚\n",
    "â”‚                            v                                                â”‚\n",
    "â”‚                          END                                                â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Agent state** flows through each node, accumulating:\n",
    "- `transcription`: Text from voice input\n",
    "- `retrieved_docs`: Relevant documents from RAG\n",
    "- `image_descriptions`: Text descriptions of any images\n",
    "- `response`: Final generated answer\n",
    "- `is_safe`: Safety check result\n",
    "\n",
    "First, let's define the state schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    # Input\n",
    "    audio_path: Optional[str]\n",
    "    text_query: Optional[str]\n",
    "    \n",
    "    # Processing\n",
    "    transcription: str\n",
    "    retrieved_docs: List[dict]\n",
    "    image_descriptions: List[str]\n",
    "    \n",
    "    # Output\n",
    "    response: str\n",
    "    safety_result: dict\n",
    "    is_safe: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow nodes\n",
    "\n",
    "def transcribe_audio(state: AgentState) -> AgentState:\n",
    "    \"\"\"Transcribe audio input to text.\"\"\"\n",
    "    if state.get(\"audio_path\"):\n",
    "        transcription = asr_model.transcribe(state[\"audio_path\"])\n",
    "    else:\n",
    "        transcription = state.get(\"text_query\", \"\")\n",
    "    \n",
    "    return {\"transcription\": transcription}\n",
    "\n",
    "\n",
    "def retrieve_documents(state: AgentState) -> AgentState:\n",
    "    \"\"\"Retrieve and rerank relevant documents.\"\"\"\n",
    "    query = state[\"transcription\"]\n",
    "    docs = retrieve_and_rerank(query, top_k=3)\n",
    "    return {\"retrieved_docs\": docs}\n",
    "\n",
    "\n",
    "def describe_images(state: AgentState) -> AgentState:\n",
    "    \"\"\"Describe any images in retrieved documents.\"\"\"\n",
    "    descriptions = []\n",
    "    query = state[\"transcription\"]\n",
    "    \n",
    "    for doc in state[\"retrieved_docs\"]:\n",
    "        if doc.get(\"image\") is not None:\n",
    "            desc = llm.describe_image(\n",
    "                doc[\"image\"],\n",
    "                f\"Describe this image in the context of: {query}\"\n",
    "            )\n",
    "            descriptions.append(f\"[Image Description]: {desc}\")\n",
    "    \n",
    "    return {\"image_descriptions\": descriptions}\n",
    "\n",
    "\n",
    "def generate_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate the final response using Nemotron 3 Nano.\"\"\"\n",
    "    \n",
    "    # Build context from retrieved docs\n",
    "    context_parts = []\n",
    "    for doc in state[\"retrieved_docs\"]:\n",
    "        context_parts.append(f\"- {doc['text']}\")\n",
    "    \n",
    "    # Add image descriptions\n",
    "    for desc in state.get(\"image_descriptions\", []):\n",
    "        context_parts.append(desc)\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {state[\"transcription\"]}\n",
    "\n",
    "Provide a clear, accurate, and helpful response based on the context above.\n",
    "\n",
    "Response:\"\"\"\n",
    "    \n",
    "    response = llm.generate(prompt, enable_thinking=True)\n",
    "    return {\"response\": response}\n",
    "\n",
    "\n",
    "def check_safety(state: AgentState) -> AgentState:\n",
    "    \"\"\"Check the safety of the response.\"\"\"\n",
    "    result = safety_guard.check_safety(\n",
    "        state[\"transcription\"],\n",
    "        state[\"response\"]\n",
    "    )\n",
    "    \n",
    "    is_safe = (\n",
    "        result.get(\"User Safety\", \"unsafe\").lower() == \"safe\" and\n",
    "        result.get(\"Response Safety\", \"safe\").lower() == \"safe\"\n",
    "    )\n",
    "    \n",
    "    return {\"safety_result\": result, \"is_safe\": is_safe}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent workflow compiled!\n"
     ]
    }
   ],
   "source": [
    "# Build the workflow graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"transcribe\", transcribe_audio)\n",
    "workflow.add_node(\"retrieve\", retrieve_documents)\n",
    "workflow.add_node(\"describe_images\", describe_images)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "workflow.add_node(\"safety_check\", check_safety)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"transcribe\")\n",
    "workflow.add_edge(\"transcribe\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"describe_images\")\n",
    "workflow.add_edge(\"describe_images\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"safety_check\")\n",
    "workflow.add_edge(\"safety_check\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "print(\"âœ… Agent workflow compiled!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the workflow (optional)\n",
    "try:\n",
    "    from IPython.display import Image as IPImage, display\n",
    "    display(IPImage(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(\"Workflow: transcribe â†’ retrieve â†’ describe_images â†’ generate â†’ safety_check â†’ END\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query: str = None, audio_path: str = None) -> dict:\n",
    "    \"\"\"Run the voice RAG agent with either text or audio input.\"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"audio_path\": audio_path,\n",
    "        \"text_query\": query,\n",
    "        \"transcription\": \"\",\n",
    "        \"retrieved_docs\": [],\n",
    "        \"image_descriptions\": [],\n",
    "        \"response\": \"\",\n",
    "        \"safety_result\": {},\n",
    "        \"is_safe\": True\n",
    "    }\n",
    "    \n",
    "    result = agent.invoke(initial_state)\n",
    "    \n",
    "    return {\n",
    "        \"query\": result[\"transcription\"],\n",
    "        \"response\": result[\"response\"],\n",
    "        \"is_safe\": result[\"is_safe\"],\n",
    "        \"safety_details\": result[\"safety_result\"],\n",
    "        \"sources\": [doc[\"id\"] for doc in result[\"retrieved_docs\"]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run the Complete Agent\n",
    "\n",
    "The agent is ready. Let's test it with both text queries and audio input.\n",
    "\n",
    "**Test flow:**\n",
    "1. Query enters the agent (text or audio)\n",
    "2. ASR transcribes audio to text (if needed)\n",
    "3. RAG retrieves and reranks relevant documents\n",
    "4. VL model describes any images in the results\n",
    "5. Reasoning model generates a response\n",
    "6. Safety guard validates the output\n",
    "7. Safe response is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Testing Voice RAG Agent\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Query: How is AI used to improve robotics?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Response: \n",
      "**AI enhances robotics in many interâ€‘related ways. Below are the main areas where AI adds capability, efficiency, and safety to robotic systems:**\n",
      "\n",
      "| AI Function | How It Improves Robotics | Typical Techniques / Examples |\n",
      "|-------------|--------------------------|------------------------------|\n",
      "| **Perception & Sensing** | Interprets raw sensor data (camera, lidar, depth, force) to understand the surrounding world. | Machineâ€‘learning vision models, sensor fusion, object detection, gesture/face...\n",
      "\n",
      "ðŸ›¡ï¸ Safety: âœ… Safe\n",
      "ðŸ“š Sources: ['doc_4', 'doc_1', 'doc_2']\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Query: What is Nemotron 3 Nano and what makes it efficient?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Response: \n",
      "**Nemotronâ€¯3â€¯Nano** is a compact, NVIDIAâ€‘developed AI model that is designed to run efficiently on devices with limited compute resourcesâ€”such as consumerâ€‘grade laptops or edge hardware (e.g., NVIDIA Jetson modules). Its primary goal is to bring advanced AI capabilities (like realâ€‘time inference on sensor streams) to everyday hardware without needing cloud services.\n",
      "\n",
      "### What makes Nemotronâ€¯3â€¯Nano efficient?\n",
      "\n",
      "| Efficiency factor | How it works |\n",
      "|-------------------|--------------|\n",
      "| **Architec...\n",
      "\n",
      "ðŸ›¡ï¸ Safety: âœ… Safe\n",
      "ðŸ“š Sources: ['doc_4', 'doc_1', 'doc_2']\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Query: Tell me about autonomous vehicles and NVIDIA DRIVE.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Response: \n",
      "**Autonomous Vehicles**\n",
      "\n",
      "- **What they are:** Selfâ€‘driving cars (also called robotic or autonomous vehicles) can sense their environment and navigate without a human driver.  \n",
      "- **How they work:**  \n",
      "  - **Sensors** â€“ cameras, radar, lidar, and other perception devices continuously collect data about the surroundings.  \n",
      "  - **AIâ€‘driven perception & decisionâ€‘making** â€“ deepâ€‘learning models process this data to detect objects, understand traffic rules, plan a safe path, and execute maneuvers such ...\n",
      "\n",
      "ðŸ›¡ï¸ Safety: âœ… Safe\n",
      "ðŸ“š Sources: ['doc_4', 'doc_1', 'doc_2']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with text input\n",
    "print(\"ðŸ¤– Testing Voice RAG Agent\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_queries = [\n",
    "    \"How is AI used to improve robotics?\",\n",
    "    \"What is Nemotron 3 Nano and what makes it efficient?\",\n",
    "    \"Tell me about autonomous vehicles and NVIDIA DRIVE.\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = run_agent(query=query)\n",
    "    \n",
    "    print(f\"ðŸ¤– Response: {result['response'][:500]}...\")\n",
    "    print(f\"\\nðŸ›¡ï¸ Safety: {'âœ… Safe' if result['is_safe'] else 'âš ï¸ Unsafe'}\")\n",
    "    print(f\"ðŸ“š Sources: {result['sources']}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-02 17:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2026-01-02 17:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¤ Testing with Audio Input\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 1it [00:00, 18.04it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Transcription: Hypothesis(score=-465.7001953125, y_sequence=tensor([112, 127,  41, 685, 342, 291,  32, 120, 143, 160, 358, 963,  54, 589,\n",
      "        977]), text='Could you please tell me about robotics?', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)\n",
      "\n",
      "ðŸ¤– Response: \n",
      "**Robotics** is a multidisciplinary field that brings together engineering, computer science, mathematics, and biology to design, build, and operate machines (robots) that can sense their environment, reason about it, and act upon it to achieve a goal. Below is a concise overview that ties together the key ideas highlighted in the context you provided.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. What Is a Robot?\n",
      "\n",
      "- **Definition**: A robot is a programmable machine that can automatically perform tasksâ€”often repetitive or hazar...\n",
      "\n",
      "ðŸ›¡ï¸ Safety: âœ… Safe\n"
     ]
    }
   ],
   "source": [
    "# Test with audio input (using the sample audio)\n",
    "print(\"\\nðŸŽ¤ Testing with Audio Input\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = run_agent(audio_path=sample_audio_path)\n",
    "\n",
    "print(f\"ðŸ“ Transcription: {result['query']}\")\n",
    "print(f\"\\nðŸ¤– Response: {result['response'][:500]}...\")\n",
    "print(f\"\\nðŸ›¡ï¸ Safety: {'âœ… Safe' if result['is_safe'] else 'âš ï¸ Unsafe'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've built a complete voice-powered RAG agent using NVIDIA Nemotron models:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        WHAT YOU BUILT                                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Component          Model                           Purpose                â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€                           â”€â”€â”€â”€â”€â”€â”€                â”‚\n",
    "â”‚   ASR                nemotron-speech-streaming       Voice â†’ Text           â”‚\n",
    "â”‚   Embeddings         llama-nemotron-embed-vl         Semantic search        â”‚\n",
    "â”‚   Reranking          llama-nemotron-rerank-vl        Sharpen accuracy       â”‚\n",
    "â”‚   Vision-Language    nemotron-nano-12b-vl            Describe images        â”‚\n",
    "â”‚   Reasoning          nemotron-3-nano-30b             Generate responses     â”‚\n",
    "â”‚   Safety             Llama-3.1-Nemotron-Safety       Content moderation     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Scale up**: Add more documents to your knowledge base\n",
    "- **Deploy**: Use NVIDIA NIM for production-ready inference\n",
    "- **Customize**: Fine-tune models on your domain-specific data\n",
    "- **Stream**: Enable real-time streaming responses\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Nemotron Models on Hugging Face](https://huggingface.co/nvidia)\n",
    "- [NVIDIA NIM](https://developer.nvidia.com/nim)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
