{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Run Bash Agent with HuggingFace Model\n\nThis notebook runs the trained CLI agent using local HuggingFace model inference.\n\n## Prerequisites\n\n- Run `02_grpo_training.ipynb` first to train and save the model\n- Model checkpoint should be at `outputs/grpo_langgraph_cli/merged_model`\n\n## Features\n\n- **Structured tool calling**: Uses JSON-based tool calls (not legacy code block parsing)\n- **Human-in-the-loop**: All commands require user confirmation before execution\n- **Security**: Command allowlist and injection protection\n- **Conversation history**: Full context maintained across turns"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add bash_agent to path for imports\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"bash_agent\"))\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data/outputs/grpo_langgraph_cli/merged_model\n",
      "Root directory: /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data\n",
      "Allowed commands: ['cd', 'cp', 'ls', 'cat', 'find', 'touch', 'echo', 'grep', 'pwd', 'mkdir', 'wget', 'sort', 'head', 'tail', 'du', 'wc', 'file', 'langgraph']\n"
     ]
    }
   ],
   "source": [
    "# Import configuration from bash_agent\n",
    "from config import Config\n",
    "\n",
    "# Create config with notebook-specific settings\n",
    "config = Config()\n",
    "\n",
    "# Override model path if needed (uncomment to change)\n",
    "# config.model_path = \"outputs/grpo_langgraph_cli/merged_model\"\n",
    "\n",
    "print(f\"Model path: {config.model_path}\")\n",
    "print(f\"Root directory: {config.root_dir}\")\n",
    "print(f\"Allowed commands: {config.allowed_commands}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import the Bash Tool\n",
    "\n",
    "Import from `bash_agent/bash.py` - provides secure command execution with:\n",
    "- Command allowlist\n",
    "- Injection protection\n",
    "- Working directory tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bash tool initialized. Working directory: /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data\n"
     ]
    }
   ],
   "source": [
    "# Import Bash tool from bash_agent\n",
    "from bash import Bash\n",
    "\n",
    "# Initialize the bash tool with security features\n",
    "bash = Bash(config)\n",
    "print(f\"Bash tool initialized. Working directory: {bash.cwd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Message Handler and LLM Interface\n",
    "\n",
    "Import from `bash_agent/helpers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Messages and HuggingFaceLLM from bash_agent.helpers\n"
     ]
    }
   ],
   "source": [
    "# Import Messages and HuggingFaceLLM from bash_agent\n",
    "from helpers import Messages, HuggingFaceLLM\n",
    "\n",
    "print(\"Imported Messages and HuggingFaceLLM from bash_agent.helpers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceLLM ready (imported from bash_agent.helpers)\n"
     ]
    }
   ],
   "source": [
    "# HuggingFaceLLM was imported above from bash_agent.helpers\n",
    "# It provides:\n",
    "#   - Local model inference with HuggingFace transformers\n",
    "#   - JSON tool call parsing from model responses\n",
    "#   - Conversion of structured commands to bash commands\n",
    "print(\"HuggingFaceLLM ready (imported from bash_agent.helpers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify model exists\nmodel_path = config.model_path\nif not os.path.exists(model_path):\n    print(f\"ERROR: Model not found at {model_path}\")\n    print(\"Please run 02_grpo_training.ipynb first to train and save the model.\")\nelse:\n    print(f\"Model found at: {model_path}\")\n    print(f\"\\nContents:\")\n    for f in os.listdir(model_path)[:10]:\n        print(f\"  - {f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data/outputs/grpo_langgraph_cli/merged_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1aeee2ca89f4da085d08803b2964127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "llm = HuggingFaceLLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the Model\n",
    "\n",
    "Quick test to verify the model generates proper JSON tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using json_system_prompt (matches training):\n",
      "You are an expert CLI assistant for the LangGraph Platform CLI.\n",
      "\n",
      "Translate user requests into structured JSON tool calls.\n",
      "\n",
      "Available commands:\n",
      "- new: Create project (flags: template, path)\n",
      "- dev: Star...\n",
      "============================================================\n",
      "\n",
      "[Query] Create a new project using the react-agent template\n",
      "[Tool Call] exec_bash_command: langgraph new --template react-agent\n",
      "----------------------------------------\n",
      "\n",
      "[Query] Start the dev server on port 8080 without opening a browser\n",
      "[Tool Call] exec_bash_command: langgraph dev --port 8080 --no-browser\n",
      "----------------------------------------\n",
      "\n",
      "[Query] Build a Docker image and tag it as myapp:v2\n",
      "[Tool Call] exec_bash_command: langgraph build -t myapp:v2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test queries - use the JSON system prompt (matches training!)\n",
    "test_queries = [\n",
    "    \"Create a new project using the react-agent template\",\n",
    "    \"Start the dev server on port 8080 without opening a browser\",\n",
    "    \"Build a Docker image and tag it as myapp:v2\",\n",
    "]\n",
    "\n",
    "# IMPORTANT: Use json_system_prompt - this is what the model was trained with!\n",
    "print(\"Using json_system_prompt (matches training):\")\n",
    "print(config.json_system_prompt[:200] + \"...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n[Query] {query}\")\n",
    "    \n",
    "    # Create fresh messages with the JSON system prompt (not generic bash prompt)\n",
    "    test_messages = Messages(config.json_system_prompt)\n",
    "    test_messages.add_user_message(query)\n",
    "    \n",
    "    response, tool_calls = llm.query(test_messages)\n",
    "    \n",
    "    # Clean up response for display\n",
    "    display_response = response\n",
    "    if \"</think>\" in display_response:\n",
    "        display_response = display_response.split(\"</think>\")[-1].strip()\n",
    "    \n",
    "    if tool_calls:\n",
    "        for tc in tool_calls:\n",
    "            args = json.loads(tc[\"function\"][\"arguments\"])\n",
    "            print(f\"[Tool Call] {tc['function']['name']}: {args.get('cmd', args)}\")\n",
    "    else:\n",
    "        print(\"[Tool Call] None detected\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interactive Agent Loop\n",
    "\n",
    "Run the full agent with human-in-the-loop confirmation.\n",
    "\n",
    "**Commands:**\n",
    "- Type your request and press Enter\n",
    "- When a command is proposed, type `y` to execute or `n` to decline\n",
    "- Type `quit` or `exit` to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loop defined. Run the next cell to start the interactive agent.\n"
     ]
    }
   ],
   "source": [
    "def confirm_execution(cmd: str) -> bool:\n",
    "    \"\"\"Ask user to confirm command execution.\"\"\"\n",
    "    response = input(f\"    Execute '{cmd}'? [y/N]: \").strip().lower()\n",
    "    return response == \"y\" or response == \"yes\"\n",
    "\n",
    "\n",
    "def run_agent_loop():\n",
    "    \"\"\"Main agent interaction loop.\"\"\"\n",
    "    \n",
    "    # Initialize conversation with JSON system prompt (matches training)\n",
    "    messages = Messages(config.json_system_prompt)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Bash Computer Use Agent (HuggingFace)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {config.model_path}\")\n",
    "    print(f\"Working directory: {bash.cwd}\")\n",
    "    print(\"Type 'quit' or 'exit' to stop.\")\n",
    "    print(\"Type 'clear' to reset conversation.\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(f\"['{bash.cwd}'] > \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\n\\nShutting down. Bye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            print(\"\\nShutting down. Bye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == \"clear\":\n",
    "            messages.clear()\n",
    "            print(\"Conversation cleared.\\n\")\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Add context about current directory\n",
    "        user_with_context = f\"{user_input}\\nCurrent working directory: `{bash.cwd}`\"\n",
    "        messages.add_user_message(user_with_context)\n",
    "        \n",
    "        # Agent loop - may involve multiple tool calls\n",
    "        while True:\n",
    "            print(\"\\nThinking...\")\n",
    "            \n",
    "            try:\n",
    "                response, tool_calls = llm.query(messages)\n",
    "            except Exception as e:\n",
    "                print(f\"Error querying model: {e}\")\n",
    "                break\n",
    "            \n",
    "            # Clean response for display\n",
    "            display_response = response.strip()\n",
    "            if \"</think>\" in display_response:\n",
    "                display_response = display_response.split(\"</think>\")[-1].strip()\n",
    "            \n",
    "            if display_response:\n",
    "                messages.add_assistant_message(display_response)\n",
    "            \n",
    "            # Process tool calls\n",
    "            if tool_calls:\n",
    "                for tc in tool_calls:\n",
    "                    function_name = tc[\"function\"][\"name\"]\n",
    "                    function_args = json.loads(tc[\"function\"][\"arguments\"])\n",
    "                    tool_id = tc[\"id\"]\n",
    "                    \n",
    "                    if function_name != \"exec_bash_command\" or \"cmd\" not in function_args:\n",
    "                        tool_result = {\"error\": \"Incorrect tool or function argument\"}\n",
    "                    else:\n",
    "                        command = function_args[\"cmd\"]\n",
    "                        print(f\"\\nProposed command: {command}\")\n",
    "                        \n",
    "                        if confirm_execution(command):\n",
    "                            tool_result = bash.exec_bash_command(command)\n",
    "                            \n",
    "                            if tool_result.get(\"stdout\"):\n",
    "                                print(f\"\\nOutput:\\n{tool_result['stdout']}\")\n",
    "                            if tool_result.get(\"stderr\"):\n",
    "                                print(f\"\\nError:\\n{tool_result['stderr']}\")\n",
    "                            if tool_result.get(\"error\"):\n",
    "                                print(f\"\\nError:\\n{tool_result['error']}\")\n",
    "                        else:\n",
    "                            tool_result = {\"error\": \"The user declined to execute this command.\"}\n",
    "                    \n",
    "                    messages.add_tool_message(json.dumps(tool_result), tool_id)\n",
    "            else:\n",
    "                # No tool calls - show assistant message and break\n",
    "                if display_response:\n",
    "                    print(f\"\\n{display_response}\")\n",
    "                print(\"-\" * 60)\n",
    "                break\n",
    "\n",
    "print(\"Agent loop defined. Run the next cell to start the interactive agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Bash Computer Use Agent (HuggingFace)\n",
      "============================================================\n",
      "Model: /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data/outputs/grpo_langgraph_cli/merged_model\n",
      "Working directory: /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data\n",
      "Type 'quit' or 'exit' to stop.\n",
      "Type 'clear' to reset conversation.\n",
      "============================================================\n",
      "\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Proposed command: langgraph new --template react-agent /home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Okay, the user asked to start a new LangGraph project, and I called the 'new' command with the template 'react-agent' and the provided path. But the tool response says the user declined to execute the command. Hmm, maybe the user didn't actually want to proceed with creating the project. I should check if there was a misunderstanding or if the user changed their mind.\n",
      "\n",
      "Wait, the user's original request was \"Can you start a new langgraph project?\" and I assumed they wanted to proceed. But the tool's response indicates a decline. Maybe the user didn't confirm or there was an error in the command execution. Alternatively, perhaps the 'new' command requires additional parameters or there's a validation step that failed. \n",
      "\n",
      "Looking at the available commands, 'new' requires template and path. I provided both, but maybe the path is invalid or the template isn't available. The user's current directory is given, so the path should be correct. Alternatively, maybe the user didn't want to create a project in that directory. \n",
      "\n",
      "Since the tool response says the user declined, perhaps the CLI has a confirmation step that wasn't handled. Or maybe the user's input was ambiguous. I should ask the user to clarify if they still want\n",
      "------------------------------------------------------------\n",
      "\n",
      "Shutting down. Bye!\n"
     ]
    }
   ],
   "source": [
    "# Start the interactive agent\n",
    "# Note: This cell requires interactive input. Stop with 'quit' or 'exit'\n",
    "run_agent_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook imports from the `bash_agent` module:\n",
    "\n",
    "| Import | Source | Description |\n",
    "|--------|--------|-------------|\n",
    "| `Config` | `bash_agent/config.py` | Model path, security settings, system prompt |\n",
    "| `Bash` | `bash_agent/bash.py` | Secure command execution with allowlist |\n",
    "| `Messages` | `bash_agent/helpers.py` | Conversation history management |\n",
    "| `HuggingFaceLLM` | `bash_agent/helpers.py` | Local model inference with JSON parsing |\n",
    "\n",
    "### bash_agent Module Structure\n",
    "\n",
    "```\n",
    "bash_agent/\n",
    "├── config.py      # Configuration class\n",
    "├── bash.py        # Bash tool with security features\n",
    "├── helpers.py     # Messages and HuggingFaceLLM classes\n",
    "├── prompts.py     # System prompts (JSON-based, no legacy)\n",
    "└── main_hf.py     # CLI entry point for the agent\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Structured tool calling**: JSON-based tool calls (not code block parsing)\n",
    "- **Human-in-the-loop**: All commands require user confirmation\n",
    "- **Security**: Command allowlist and injection protection\n",
    "- **Conversation history**: Full context maintained across turns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Train longer for better accuracy (increase `max_steps` in training)\n",
    "- Add more commands to the allowlist in `bash_agent/config.py`\n",
    "- Run from CLI: `python bash_agent/main_hf.py`\n",
    "- Deploy as a server with vLLM for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}