{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Train a CLI Agent with GRPO using Unsloth and NeMo Gym\n",
    "\n",
    "This notebook trains **Nemotron-3-Nano** to translate natural language into LangGraph CLI commands using:\n",
    "- **Unsloth** for efficient training\n",
    "- **NeMo Gym** resource server for verifiable rewards\n",
    "- **GRPO** (Group Relative Policy Optimization) for RL training\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Start the NeMo Gym resource server for CLI verification\n",
    "2. Load Nemotron-3-Nano\n",
    "3. Create a reward function that calls the NeMo Gym `/verify` endpoint\n",
    "4. Train with GRPO\n",
    "5. Save the trained model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Start the NeMo Gym Resource Server\n",
    "\n",
    "Before training, start the LangGraph CLI resource server in a separate terminal:\n",
    "\n",
    "```bash\n",
    "cd nemo_gym_resources/langgraph_cli\n",
    "uv run uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "You should see:\n",
    "```\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000\n",
    "INFO:     Application startup complete.\n",
    "```\n",
    "\n",
    "The server provides a `/verify` endpoint that evaluates model outputs against expected CLI commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeMo Gym server is running at 127.0.0.1:8000\n",
      "Verify endpoint: http://127.0.0.1:8000/verify\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# NeMo Gym resource server configuration\n",
    "NEMO_GYM_HOST = \"127.0.0.1\"\n",
    "NEMO_GYM_PORT = 8000\n",
    "VERIFY_ENDPOINT = f\"http://{NEMO_GYM_HOST}:{NEMO_GYM_PORT}/verify\"\n",
    "\n",
    "# Verify server is running\n",
    "try:\n",
    "    response = requests.get(f\"http://{NEMO_GYM_HOST}:{NEMO_GYM_PORT}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"NeMo Gym server is running at {NEMO_GYM_HOST}:{NEMO_GYM_PORT}\")\n",
    "        print(f\"Verify endpoint: {VERIFY_ENDPOINT}\")\n",
    "    else:\n",
    "        print(f\"Server responded with status {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"ERROR: NeMo Gym server is not running!\")\n",
    "    print(\"Please start it with: uvicorn app:app --host 0.0.0.0 --port 8000\")\n",
    "    print(\"(from the nemo_gym_resources/langgraph_cli directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-14 19:58:36 [__init__.py:241] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: Could not import trl.trainer.alignprop_trainer: Failed to import trl.trainer.alignprop_trainer because of the following error (look up to see its traceback):\n",
      "cannot import name 'DDPOStableDiffusionPipeline' from 'trl.models' (/home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data/.venv/lib/python3.13/site-packages/trl/models/__init__.py)\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "VRAM: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"  # Disable torch.compile for stability\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Load Nemotron-Nano-9B with Unsloth + Monkey Patch\n",
    "\n",
    "Nemotron-Nano-9B-v2 is a hybrid Transformer-Mamba2 model (~9B parameters).\n",
    "\n",
    "**Important:** Nemotron uses custom modeling code (`modeling_nemotron_h.py`) that doesn't respect Unsloth's `UNSLOTH_RETURN_HIDDEN_STATES` environment variable. We apply a monkey-patch to fix this.\n",
    "\n",
    "We use BF16 (no 4-bit quantization) because Mamba2 CUDA kernels don't support quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: nvidia/NVIDIA-Nemotron-Nano-9B-v2\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2026.1.2: Fast Nemotron patching. Transformers: 4.56.2. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Nemotron does not support SDPA - switching to fast eager.\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2a2941fd2448a8b663b6a8ed645218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia/NVIDIA-Nemotron-Nano-9B-v2 does not have a padding token! Will use pad_token = <SPECIAL_999>.\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LORA_RANK = 16\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# NOTE: Nemotron-Nano-9B-v2 is a hybrid Transformer-Mamba2 model.\n",
    "# The Mamba2 CUDA kernels are NOT compatible with bitsandbytes 4-bit quantization.\n",
    "# Use BF16 instead - H100 80GB has plenty of VRAM.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,  # Disabled: Mamba2 kernels incompatible with 4-bit\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Patched NemotronHForCausalLM.forward for Unsloth GRPO compatibility\n",
      "Unsloth: Making `model.base_model.model.backbone` require gradients\n",
      "LoRA adapters applied (rank=16)\n"
     ]
    }
   ],
   "source": [
    "# Apply the Nemotron monkey-patch for Unsloth GRPO compatibility\n",
    "from nemotron_unsloth_patch import patch_nemotron_for_unsloth_grpo\n",
    "\n",
    "patch_nemotron_for_unsloth_grpo(model)\n",
    "\n",
    "# Apply LoRA adapters for parameter-efficient training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_RANK * 2,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"LoRA adapters applied (rank={LORA_RANK})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Load Training Dataset\n",
    "\n",
    "Load synthetic CLI data. Each record contains:\n",
    "- `input`: Natural language request\n",
    "- `output`: Expected structured CLI tool call (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for CLI tool-calling\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert CLI assistant for the LangGraph Platform CLI.\n",
    "\n",
    "Translate user requests into structured JSON tool calls.\n",
    "\n",
    "Available commands:\n",
    "- new: Create project (flags: template, path)\n",
    "- dev: Start dev server (flags: port, no_browser)\n",
    "- up: Launch container (flags: port, watch)\n",
    "- build: Build image (flags: tag)\n",
    "- dockerfile: Generate Dockerfile (flags: output_path)\n",
    "\n",
    "Example: {\"command\": \"new\", \"template\": \"react-agent\", \"path\": null, \"port\": null, \"no_browser\": null, \"watch\": null, \"tag\": null, \"output_path\": null}\n",
    "\n",
    "Respond with ONLY a JSON object. Set unused flags to null.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 205 training samples\n",
      "Loaded 25 validation samples\n"
     ]
    }
   ],
   "source": [
    "def load_training_data(filepath: str) -> Dataset:\n",
    "      data = []\n",
    "\n",
    "      with open(filepath, \"r\") as f:\n",
    "          for line in f:\n",
    "              ex = json.loads(line)\n",
    "\n",
    "              # Format as conversation with prefilled </think>\n",
    "              prompt = [\n",
    "                  {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                  {\"role\": \"user\", \"content\": ex[\"input\"]},\n",
    "              ]\n",
    "\n",
    "              output = ex[\"output\"]\n",
    "              if isinstance(output, str):\n",
    "                  output = json.loads(output)\n",
    "\n",
    "              data.append({\n",
    "                  \"prompt\": prompt,\n",
    "                  \"answer\": output,\n",
    "                  \"user_input\": ex[\"input\"],\n",
    "              })\n",
    "\n",
    "      return Dataset.from_list(data)\n",
    "\n",
    "# Load synthetic training and validation data\n",
    "TRAIN_FILE = \"data/langgraph_cli/train.jsonl\"\n",
    "VAL_FILE = \"data/langgraph_cli/val.jsonl\"\n",
    "\n",
    "train_dataset = load_training_data(TRAIN_FILE)\n",
    "val_dataset = load_training_data(VAL_FILE)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples\")\n",
    "print(f\"Loaded {len(val_dataset)} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Create Reward Function Using NeMo Gym Server\n",
    "\n",
    "The reward function calls the NeMo Gym resource server's `/verify` endpoint to evaluate model outputs.\n",
    "\n",
    "What happens in NeMo Gym during training:\n",
    "- Send model response to the verification server\n",
    "- Server computes reward based on correctness\n",
    "- Return reward for GRPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward function created using NeMo Gym endpoint: http://127.0.0.1:8000/verify\n"
     ]
    }
   ],
   "source": [
    "def create_nemo_gym_reward_function(dataset: Dataset, verify_endpoint: str):\n",
    "    \"\"\"\n",
    "    Create a reward function that uses the NeMo Gym verify endpoint.\n",
    "    \n",
    "    This mirrors the pattern from nemo_gym_sudoku.ipynb where rewards\n",
    "    are computed by sending requests to the NeMo Gym resource server.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build lookup from user query to expected output\n",
    "    query_to_expected = {}\n",
    "    for example in dataset:\n",
    "        query_to_expected[example[\"user_input\"]] = example[\"answer\"]\n",
    "    \n",
    "    def reward_fn(completions, prompts=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute rewards by calling NeMo Gym verify endpoint.\n",
    "        \n",
    "        Args:\n",
    "            completions: List of model completions\n",
    "            prompts: List of prompts (used to look up expected answers)\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of reward values\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for i, completion in enumerate(completions):\n",
    "            # Extract completion text\n",
    "            completion_text = completion[0][\"content\"] if completion else \"\"\n",
    "            \n",
    "            # Get user query from prompt\n",
    "            user_query = None\n",
    "            if prompts is not None:\n",
    "                for msg in prompts[i]:\n",
    "                    if msg[\"role\"] == \"user\":\n",
    "                        user_query = msg[\"content\"]\n",
    "                        break\n",
    "            \n",
    "            if user_query is None or user_query not in query_to_expected:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            expected = query_to_expected[user_query]\n",
    "            \n",
    "            # Prepare verify request in NeMo Gym format\n",
    "            verify_request = {\n",
    "                \"task_id\": f\"train-{i}\",\n",
    "                \"task_input\": {\n",
    "                    \"input\": user_query,\n",
    "                    \"output\": expected,\n",
    "                },\n",
    "                \"model_response\": completion_text,\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Send verify request to NeMo Gym resource server\n",
    "                resp = requests.post(verify_endpoint, json=verify_request, timeout=30)\n",
    "                \n",
    "                if resp.status_code == 200:\n",
    "                    result = resp.json()\n",
    "                    reward = result.get(\"reward\", 0.0)\n",
    "                else:\n",
    "                    reward = 0.0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                reward = 0.0\n",
    "            \n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return np.array(rewards)\n",
    "    \n",
    "    return reward_fn\n",
    "\n",
    "\n",
    "# Create the reward function bound to our dataset and server\n",
    "reward_fn = create_nemo_gym_reward_function(train_dataset, VERIFY_ENDPOINT)\n",
    "\n",
    "print(f\"Reward function created using NeMo Gym endpoint: {VERIFY_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Test the Reward Function\n",
    "\n",
    "Verify that the reward function correctly communicates with the NeMo Gym server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Response: {\n",
      "  \"task_id\": \"test-1\",\n",
      "  \"reward\": 1.0,\n",
      "  \"exact_match\": true,\n",
      "  \"command_correct\": true,\n",
      "  \"flag_accuracy\": 1.0,\n",
      "  \"feedback\": \"Exact match! Command and all flags are correct.\",\n",
      "  \"parsed_output\": {\n",
      "    \"command\": \"build\",\n",
      "    \"tag\": \"myapp:latest\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test the verify endpoint directly\n",
    "test_request = {\n",
    "    \"task_id\": \"test-1\",\n",
    "    \"task_input\": {\n",
    "        \"input\": \"Build a Docker image and tag it as myapp:latest\",\n",
    "        \"output\": {\"command\": \"build\", \"tag\": \"myapp:latest\"}\n",
    "    },\n",
    "    \"model_response\": '{\"command\": \"build\", \"tag\": \"myapp:latest\"}'\n",
    "}\n",
    "\n",
    "try:\n",
    "    resp = requests.post(VERIFY_ENDPOINT, json=test_request, timeout=10)\n",
    "    print(f\"Status: {resp.status_code}\")\n",
    "    print(f\"Response: {json.dumps(resp.json(), indent=2)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure the NeMo Gym server is running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 6: Configure and Run GRPO Training\n",
    "\n",
    "**GRPO (Group Relative Policy Optimization)** samples multiple completions per prompt\n",
    "and reinforces outputs that perform better relative to the group.\n",
    "\n",
    "The NeMo Gym server provides consistent, verifiable rewards for each completion.\n",
    "\n",
    "**Note:** With our monkey-patch applied, Unsloth's efficient GRPO implementation now works with Nemotron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e94dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[transformers_modules.nvidia.NVIDIA-Nemotron-Nano-9B-v2.a4fb57926db83548381949a9d542f9f3d71d1fd6.modeling_nemotron_h|WARNING]NemotronH requires an initialized `NemotronHHybridDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Monkey-patch verified: model returns hidden states when UNSLOTH_RETURN_HIDDEN_STATES=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the monkey-patch is working\n",
    "from nemotron_unsloth_patch import verify_patch\n",
    "verify_patch(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 205\n",
      "Max completion length: 819\n"
     ]
    }
   ],
   "source": [
    "# Calculate prompt length for config\n",
    "max_prompt_length = 0\n",
    "for example in train_dataset:\n",
    "    prompt_tokens = tokenizer.apply_chat_template(\n",
    "        example[\"prompt\"],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "    )\n",
    "    max_prompt_length = max(max_prompt_length, len(prompt_tokens))\n",
    "\n",
    "MAX_PROMPT_LENGTH = max_prompt_length + 16  # Small buffer\n",
    "MAX_COMPLETION_LENGTH = MAX_SEQ_LENGTH - MAX_PROMPT_LENGTH\n",
    "\n",
    "print(f\"Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"Max completion length: {MAX_COMPLETION_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO configuration:\n",
      "  - Generations per prompt: 4\n",
      "  - Max steps: 50\n",
      "  - Learning rate: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# GRPO Training Configuration\n",
    "training_args = GRPOConfig(\n",
    "    # Generation settings\n",
    "    temperature=1.0,\n",
    "    num_generations=4,  # Completions per prompt for GRPO\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,\n",
    "    \n",
    "    # Training settings\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=50,  # Increase for better results\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # GRPO settings\n",
    "    epsilon_high=0.28,\n",
    "    mask_truncated_completions=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=1,\n",
    "    save_steps=50,\n",
    "    output_dir=\"outputs/grpo_langgraph_cli\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"GRPO configuration:\")\n",
    "print(f\"  - Generations per prompt: {training_args.num_generations}\")\n",
    "print(f\"  - Max steps: {training_args.max_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO Trainer ready!\n",
      "Training on 205 samples\n",
      "Rewards computed via NeMo Gym server at http://127.0.0.1:8000/verify\n"
     ]
    }
   ],
   "source": [
    "# Create GRPO Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[reward_fn],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "print(\"GRPO Trainer ready!\")\n",
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "print(f\"Rewards computed via NeMo Gym server at {VERIFY_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting GRPO training...\")\n",
    "print(\"Rewards should increase as the model learns correct CLI commands.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 7: Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained model...\n",
      "============================================================\n",
      "\n",
      "[User] Create a new project using the react-agent template\n",
      "[Model] Okay, the user wants to create a new project using the react-agent template. Let me check the available commands. The 'new' command is for creating a project, and it has flags like template and path. The user specified the template as react-agent, so I should set that. The path isn't mentioned, so it should be null. The other flags like port, no_browser, watch, tag, and output_path aren't relevant here, so they should be null. I need to make sure the JSON only includes the necessary fields and sets unused ones to null. Let me structure the JSON accordingly.\n",
      "</think>\n",
      "\n",
      "{\"command\": \"new\", \"template\": \"react-agent\", \"path\": null, \"port\": null, \"no_browser\": null, \"watch\": null, \"tag\": null, \"output_path\": null}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "[User] Start the dev server on port 8080 without opening a browser\n",
      "[Model] Okay, let's see. The user wants to start the dev server on port 8080 without opening a browser.\n",
      "\n",
      "Looking at the available commands, the 'dev' command is the right one here. The flags for 'dev' are port and no_browser. \n",
      "\n",
      "The user specified port 8080, so I should set \"port\": 8080. They also mentioned not opening a browser, which corresponds to the 'no_browser' flag. So \"no_browser\": true. \n",
      "\n",
      "Other flags like template, path, watch, tag, and output_path aren't mentioned, so they should be null. \n",
      "\n",
      "Double-checking the example JSON structure, all unused flags are set to null. So the final JSON should have command \"dev\", port 8080, no_browser true, and others null.\n",
      "</think>\n",
      "\n",
      "{\"command\": \"dev\", \"port\": 8080, \"no_browser\": true, \"template\": null, \"path\": null, \"watch\": null, \"tag\": null, \"output_path\": null}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "[User] Build a Docker image and tag it as myapp:v2\n",
      "[Model] Okay, let's see. The user wants to build a Docker image and tag it as myapp:v2.\n",
      "\n",
      "Looking at the available commands, the 'build' command is the right one here. The 'build' command has a flag called 'tag', which should be set to 'myapp:v2'. The other flags like port, no_browser, watch, etc., aren't relevant for this command, so they should be set to null. The other parameters like template, path, output_path are also not needed here. So the JSON should only include the 'tag' flag under the 'build' command. I need to make sure that unused flags are set to null to match the example structure. That should cover it.\n",
      "</think>\n",
      "\n",
      "{\"command\": \"build\", \"tag\": \"myapp:v2\"}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "[User] Launch the server on port 3000 with file watching\n",
      "[Model] Okay, the user wants to launch the server on port 3000 with file watching. Let me check the available commands. The 'up' command is for launching the container, and it has flags for port and watch. So I need to set port to 3000 and watch to true. The other flags like no_browser aren't mentioned, so they should be null. Let me make sure the JSON structure matches the example. The command should be \"up\", port 3000, watch true. Other flags are optional and not provided, so they stay null. That should cover it.\n",
      "</think>\n",
      "\n",
      "{\"command\": \"up\", \"port\": 3000, \"watch\": true, \"no_browser\": null}\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Create a new project using the react-agent template\",\n",
    "    \"Start the dev server on port 8080 without opening a browser\",\n",
    "    \"Build a Docker image and tag it as myapp:v2\",\n",
    "    \"Launch the server on port 3000 with file watching\",\n",
    "]\n",
    "\n",
    "print(\"Testing trained model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n[User] {query}\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"[Model] {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 8: Save the Trained Model Locally\n",
    "\n",
    "Save the merged model (base + LoRA weights) locally for use with the CLI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving merged model to: outputs/grpo_langgraph_cli/merged_model\n",
      "Found HuggingFace hub cache directory: /home/ubuntu/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070c251dc13e4788931b62c383794d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `outputs/grpo_langgraph_cli/merged_model`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `outputs/grpo_langgraph_cli/merged_model`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 71392.41it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:29<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/ubuntu/Build_a_Computer_Use_Agent_with_Synthetic_Data/outputs/grpo_langgraph_cli/merged_model`\n",
      "Model saved successfully!\n",
      "\n",
      "To use with the CLI agent, update the model path in bash_agent/agent.py\n"
     ]
    }
   ],
   "source": [
    "# Save merged model locally using Unsloth's efficient saving\n",
    "OUTPUT_DIR = \"outputs/grpo_langgraph_cli/merged_model\"\n",
    "\n",
    "print(f\"Saving merged model to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save in 16-bit format for inference\n",
    "model.save_pretrained_merged(\n",
    "    OUTPUT_DIR,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"\\nTo use with the CLI agent, update the model path in bash_agent/hf_main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **NeMo Gym Integration** - Connected to the resource server's `/verify` endpoint for reward computation\n",
    "2. **Unsloth + Nemotron Fix** - Applied `nemotron_unsloth_patch.py` to support `UNSLOTH_RETURN_HIDDEN_STATES`\n",
    "3. **GRPO Training** - Reinforcement learning with verifiable rewards using Unsloth's efficient trainer\n",
    "4. **Local Model Saving** - Saved merged model for deployment\n",
    "\n",
    "**Key Fix:** Nemotron uses custom modeling code that doesn't respect Unsloth's `UNSLOTH_RETURN_HIDDEN_STATES` environment variable. The `nemotron_unsloth_patch.py` module applies a monkey-patch to make it compatible.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  GRPO Trainer   â”‚â”€â”€â”€â”€â–¶â”‚  NeMo Gym Server     â”‚\n",
    "â”‚  (Unsloth)      â”‚      â”‚  /verify endpoint    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                          â”‚\n",
    "         â”‚  completions             â”‚  rewards\n",
    "         â”‚                          â”‚\n",
    "         â–¼                          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Training Loop                     â”‚\n",
    "â”‚  1. Generate completions for prompts        â”‚\n",
    "â”‚  2. Send to NeMo Gym for verification       â”‚\n",
    "â”‚  3. Compute GRPO loss from rewards          â”‚\n",
    "â”‚  4. Update model weights (LoRA only)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Use the saved model in `bash_agent/agent.py` for human-in-the-loop execution\n",
    "- Generate more synthetic training data with NeMo Data Designer\n",
    "- Increase training steps for better performance\n",
    "\n",
    "## References\n",
    "\n",
    "- [NeMo Gym Documentation](https://docs.nvidia.com/nemo/gym/latest/index.html)\n",
    "- [TRL GRPO Documentation](https://huggingface.co/docs/trl/main/en/grpo_trainer)\n",
    "- [GRPO Paper](https://arxiv.org/abs/2402.03300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
